import json
import os
import time
import uuid
import asyncio
import concurrent.futures
import random
import torch  # ç”¨äºGPUæ£€æµ‹
import numpy as np  # æ–°å¢ï¼šBM25ä¾èµ–
from rank_bm25 import BM25Okapi  # æ–°å¢ï¼šBM25å…³é”®è¯æ£€ç´¢
from typing import List, Dict, Any, Optional, Tuple
from tqdm import tqdm
import sys
import re
from collections import defaultdict

sys.path.append(os.path.dirname(__file__))
from get_text_embedding import get_text_embedding

# ç¡®ä¿ä¾èµ–å®‰è£…æç¤ºï¼ˆæ–°å¢rank-bm25ï¼‰
required_packages = {
    "python-dotenv": "pip install python-dotenv",
    "openai": "pip install openai",
    "redis": "pip install redis",
    "pymilvus": "pip install pymilvus==2.4.3",  # Milvus Python SDK
    "sentence-transformers": "pip install sentence-transformers",  # å¤‡ç”¨é‡æ’åºæ¨¡å‹
    "scikit-learn": "pip install scikit-learn",  # å…œåº•å…³é”®è¯æ£€ç´¢ç”¨
    "FlagEmbedding": "pip install FlagEmbedding",  # BGEé‡æ’æ¨¡å‹ä¾èµ–
    "rank-bm25": "pip install rank-bm25"  # æ–°å¢ï¼šBM25å…³é”®è¯æ£€ç´¢
}
for pkg, install_cmd in required_packages.items():
    try:
        __import__(pkg.replace("-", "_"))
    except ImportError:
        print(f"è­¦å‘Šï¼šæœªå®‰è£…{pkg}ï¼Œè¯·æ‰§è¡Œå‘½ä»¤ï¼š{install_cmd}")

# å¯¼å…¥é‡æ’æ¨¡å‹
from FlagEmbedding import FlagReranker

# åŸºç¡€ä¾èµ–å¯¼å…¥
try:
    from dotenv import load_dotenv
except ImportError:
    load_dotenv = lambda: None

try:
    from openai import OpenAI
except ImportError:
    class OpenAI:
        def __init__(self, *args, **kwargs):
            raise ImportError("è¯·å®‰è£…openaiåŒ…ï¼špip install openai")

        def chat(self, *args, **kwargs):
            pass

try:
    import redis
except ImportError:
    class MockRedis:
        def __init__(self, *args, **kwargs):
            self.data = {}

        def setex(self, key, expire, value):
            self.data[key] = (value, time.time() + expire)

        def get(self, key):
            val, exp = self.data.get(key, (None, 0))
            return val if time.time() < exp else None


    redis = MockRedis

# Milvusç›¸å…³å¯¼å…¥ï¼ˆå‘é‡åº“ä¼˜åŒ–æ ¸å¿ƒï¼‰
try:
    from pymilvus import (
        connections,
        FieldSchema, CollectionSchema, DataType,
        Collection,
        utility,
        Partition
    )

    MILVUS_AVAILABLE = True
except ImportError:
    MILVUS_AVAILABLE = False
    print("âš ï¸ Milvusæœªå®‰è£…ï¼Œå°†å›é€€åˆ°SimpleVectorStoreï¼ˆæ€§èƒ½è¾ƒå·®ï¼‰")

# å…³é”®è¯æ£€ç´¢ä¾èµ–ï¼ˆå…œåº•æœºåˆ¶ç”¨ï¼‰
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity

    TFIDF_AVAILABLE = True
except ImportError:
    TFIDF_AVAILABLE = False
    print("âš ï¸ scikit-learnæœªå®‰è£…ï¼Œå…œåº•å…³é”®è¯æ£€ç´¢åŠŸèƒ½å—é™")

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


# ===================== 1. å®¡è®¡æ—¥å¿—å·¥å…·ç±»ï¼ˆå¢å¼ºé‡è¯•æœºåˆ¶ï¼‰ =====================
class AuditLogManager:
    def __init__(self, redis_host: str = "localhost", redis_port: int = 6379, redis_db: int = 0):
        try:
            self.redis_client = redis.Redis(
                host=redis_host, port=redis_port, db=redis_db, decode_responses=True
            )
            self.redis_client.ping()
            self.using_redis = True
            print("âœ… Rediså®¡è®¡æ—¥å¿—è¿æ¥æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ Redisè¿æ¥å¤±è´¥ï¼Œä½¿ç”¨å†…å­˜æ¨¡æ‹Ÿ: {str(e)}")
            self.redis_client = {}
            self.using_redis = False

        self.db_config = {
            "host": os.getenv("DB_HOST", "localhost"),
            "port": os.getenv("DB_PORT", 3306),
            "user": os.getenv("DB_USER", "root"),
            "password": os.getenv("DB_PASSWORD", ""),
            "database": os.getenv("AUDIT_DB_NAME", "audit_logs")
        }

    def create_pre_log(self, question: str, metadata: Optional[Dict[str, Any]] = None) -> str:
        log_id = uuid.uuid4().hex
        log_data = {
            "log_id": log_id,
            "question": question,
            "status": "processing",
            "timestamp": time.time(),
            "create_time": time.strftime("%Y-%m-%d %H:%M:%S"),
            "metadata": metadata or {}
        }
        if self.using_redis:
            self.redis_client.setex(f"audit_log:{log_id}", 3600, json.dumps(log_data, ensure_ascii=False))
        else:
            self.redis_client[f"audit_log:{log_id}"] = (log_data, time.time() + 3600)
        return log_id

    def update_log_with_answer(self, log_id: str, answer: str, retrieval_chunks: List[Dict[str, Any]],
                               status: str = "completed", model_version: str = "unknown",
                               retrieval_type: str = "vector") -> bool:
        """å¢å¼ºï¼šè®°å½•æ¨¡å‹ç‰ˆæœ¬å’Œæ£€ç´¢ç±»å‹ï¼ˆå‘é‡/å…œåº•ï¼‰"""
        if self.using_redis:
            log_str = self.redis_client.get(f"audit_log:{log_id}")
            if not log_str:
                print(f"âŒ æ—¥å¿—ID {log_id} ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ")
                return False
            log_data = json.loads(log_str)
        else:
            log_entry = self.redis_client.get(f"audit_log:{log_id}")
            if not log_entry or time.time() > log_entry[1]:
                print(f"âŒ æ—¥å¿—ID {log_id} ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ")
                return False
            log_data = log_entry[0]

        log_data.update({
            "answer": answer,
            "retrieval_chunks": [
                {"id": c.get("id"), "metadata": c.get("metadata"), "model_version": c.get("model_version", "unknown")}
                for c in retrieval_chunks],
            "status": status,
            "complete_time": time.strftime("%Y-%m-%d %H:%M:%S"),
            "process_duration": round(time.time() - log_data["timestamp"], 3),
            "embedding_model_version": model_version,  # è®°å½•åµŒå…¥æ¨¡å‹ç‰ˆæœ¬
            "retrieval_type": retrieval_type  # è®°å½•æ£€ç´¢ç±»å‹ï¼ˆvector/keyword_fallbackï¼‰
        })

        if self.using_redis:
            self.redis_client.setex(f"audit_log:{log_id}", 86400, json.dumps(log_data, ensure_ascii=False))
        else:
            self.redis_client[f"audit_log:{log_id}"] = (log_data, time.time() + 86400)
        return True

    async def write_audit_log_to_db(self, log_id: str, retries: int = 3) -> None:
        """æ–°å¢ï¼šå¼‚æ­¥è½ç›˜å¸¦é‡è¯•æœºåˆ¶"""
        for attempt in range(retries):
            try:
                if self.using_redis:
                    log_str = self.redis_client.get(f"audit_log:{log_id}")
                    if not log_str:
                        print(f"âŒ å¼‚æ­¥è½ç›˜å¤±è´¥ï¼šæ—¥å¿—ID {log_id} ä¸å­˜åœ¨")
                        return
                    log_data = json.loads(log_str)
                else:
                    log_entry = self.redis_client.get(f"audit_log:{log_id}")
                    if not log_entry or time.time() > log_entry[1]:
                        print(f"âŒ å¼‚æ­¥è½ç›˜å¤±è´¥ï¼šæ—¥å¿—ID {log_id} ä¸å­˜åœ¨")
                        return
                    log_data = log_entry[0]

                print(f"ğŸ“ å¼‚æ­¥è½ç›˜æ—¥å¿— {log_id} åˆ°æ•°æ®åº“: {log_data['question'][:20]}...")
                await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿæ•°æ®åº“å»¶è¿Ÿ
                if self.using_redis:
                    self.redis_client.delete(f"audit_log:{log_id}")
                break  # æˆåŠŸåˆ™é€€å‡ºé‡è¯•
            except Exception as e:
                if attempt < retries - 1:
                    print(f"âš ï¸ å¼‚æ­¥è½ç›˜å°è¯• {attempt+1} å¤±è´¥ï¼Œé‡è¯•ä¸­: {str(e)}")
                    await asyncio.sleep(1)  # é‡è¯•é—´éš”1ç§’
                else:
                    print(f"âŒ å¼‚æ­¥è½ç›˜æœ€ç»ˆå¤±è´¥ï¼ˆ{retries}æ¬¡å°è¯•ï¼‰: {str(e)}")
                    if self.using_redis:
                        self.redis_client.lpush("audit_log_failed", log_id)


# ===================== 2. Chunkå¤„ç†å·¥å…·ï¼ˆä¿ç•™åŸæœ‰åŠŸèƒ½ï¼‰ =====================
def smart_truncate(text: str, max_tokens: int = 512) -> str:
    tokens = text.split()
    if len(tokens) <= max_tokens:
        return text
    number_positions = [i for i, token in enumerate(tokens) if any(c.isdigit() for c in token)]
    center_idx = number_positions[len(number_positions) // 2] if number_positions else len(tokens) // 2
    half_window = max_tokens // 2
    start_idx = max(0, center_idx - half_window)
    end_idx = min(len(tokens), start_idx + max_tokens)
    return " ".join(tokens[start_idx:end_idx])


def slide_window_split(text: str, max_tokens: int = 512, slide_step: int = 200) -> List[str]:
    tokens = text.split()
    total_tokens = len(tokens)
    if total_tokens <= max_tokens:
        return [text]
    sub_chunks = []
    start_idx = 0
    while start_idx < total_tokens:
        end_idx = min(start_idx + max_tokens, total_tokens)
        sub_tokens = tokens[start_idx:end_idx]
        sub_chunk = " ".join(sub_tokens)
        sub_chunks.append(smart_truncate(sub_chunk, max_tokens))
        start_idx += slide_step
        if start_idx + max_tokens > total_tokens and start_idx < total_tokens:
            start_idx = max(0, total_tokens - max_tokens)
    return list(dict.fromkeys(sub_chunks))


def process_long_chunk(chunk: Dict[str, Any], max_tokens: int = 512, slide_step: int = 200) -> List[Dict[str, Any]]:
    raw_content = chunk["content"]
    metadata = chunk["metadata"].copy()
    sub_contents = slide_window_split(raw_content, max_tokens, slide_step)
    sub_chunks = []
    for idx, sub_content in enumerate(sub_contents):
        sub_metadata = metadata.copy()
        sub_metadata["sub_chunk_idx"] = idx
        sub_metadata["total_sub_chunks"] = len(sub_contents)
        sub_chunks.append({"content": sub_content, "metadata": sub_metadata})
    return sub_chunks


# ===================== 3. å‘é‡åº“ä¼˜åŒ–ï¼šMilvusVectorStoreï¼ˆå¢å¼ºç¼“å­˜ç­–ç•¥ï¼‰ =====================
class MilvusVectorStore:
    """å¢å¼ºç‰ˆMilvuså‘é‡åº“ï¼šæ”¯æŒæ¨¡å‹ç‰ˆæœ¬ç»‘å®šå’Œç¼“å­˜ä¼˜åŒ–"""

    def __init__(self, collection_name: str = "rag_finance_chunks", dim: int = 1536):
        self.collection_name = collection_name
        self.dim = dim
        self.collection = None
        self.embedding_model_version = None  # æ–°å¢ï¼šç¼“å­˜å½“å‰æ¨¡å‹ç‰ˆæœ¬
        self._connect_milvus()
        self._create_collection_with_version()  # åˆ›å»ºå«ç‰ˆæœ¬å­—æ®µçš„é›†åˆ
        self._load_all_versions()  # åŠ è½½ç°æœ‰æ‰€æœ‰æ¨¡å‹ç‰ˆæœ¬

    def get_cache_key(self, query: str) -> str:
        """æ–°å¢ï¼šå¸¦æ¨¡å‹ç‰ˆæœ¬çš„ç¼“å­˜é”®ç”Ÿæˆ"""
        return f"embedding:{self.embedding_model_version}:{hash(query)}"

    def set_embedding_version(self, version: str) -> None:
        """è®¾ç½®å½“å‰åµŒå…¥æ¨¡å‹ç‰ˆæœ¬ï¼ˆç”¨äºç¼“å­˜é”®ï¼‰"""
        self.embedding_model_version = version

    def _connect_milvus(self) -> None:
        if not MILVUS_AVAILABLE:
            self.collection = None
            return
        milvus_host = os.getenv("MILVUS_HOST", "localhost")
        milvus_port = os.getenv("MILVUS_PORT", "19530")
        try:
            connections.connect(
                alias="default",
                host=milvus_host,
                port=milvus_port
            )
            print(f"âœ… æˆåŠŸè¿æ¥Milvusï¼š{milvus_host}:{milvus_port}")
        except Exception as e:
            print(f"âš ï¸ Milvusè¿æ¥å¤±è´¥ï¼Œå°†å›é€€åˆ°å†…å­˜æ¨¡å¼ï¼š{str(e)}")
            self.collection = None
            self.in_memory_chunks = []  # å†…å­˜æ¨¡å¼ä¸‹å­˜å‚¨å¸¦ç‰ˆæœ¬çš„chunk
            self.in_memory_embeddings = []
            self.in_memory_versions = []  # å†…å­˜æ¨¡å¼ä¸‹å­˜å‚¨ç‰ˆæœ¬ä¿¡æ¯

    def _create_collection_with_version(self) -> None:
        """åˆ›å»ºåŒ…å«æ¨¡å‹ç‰ˆæœ¬å­—æ®µçš„é›†åˆ"""
        if not MILVUS_AVAILABLE or self.collection:
            return
        if not utility.has_collection(self.collection_name, using="default"):
            # å­—æ®µå®šä¹‰å¢å¼ºï¼šæ–°å¢embedding_model_versionå­—æ®µ
            fields = [
                FieldSchema(name="id", dtype=DataType.VARCHAR, is_primary=True, max_length=64),
                FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=self.dim),
                FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=4096),
                FieldSchema(name="metadata", dtype=DataType.JSON),
                FieldSchema(name="embedding_model_version", dtype=DataType.VARCHAR, max_length=64)  # æ¨¡å‹ç‰ˆæœ¬æ ‡ç­¾
            ]
            schema = CollectionSchema(fields=fields, description="RAGé‡‘èåœºæ™¯Chunkå‘é‡é›†åˆï¼ˆå¸¦ç‰ˆæœ¬ç»‘å®šï¼‰")
            self.collection = Collection(name=self.collection_name, schema=schema, using="default")

            # ä¿æŒåŸæœ‰IVF_HNSWç´¢å¼•é…ç½®
            index_params = {
                "index_type": "IVF_HNSW",
                "metric_type": "IP",
                "params": {"nlist": 2048, "M": 8, "efConstruction": 64}
            }
            self.collection.create_index(field_name="embedding", index_params=index_params)
            print(f"âœ… Milvusé›†åˆ[{self.collection_name}]åŠç‰ˆæœ¬å­—æ®µåˆ›å»ºå®Œæˆ")
        else:
            self.collection = Collection(name=self.collection_name, using="default")
            # æ£€æŸ¥ç°æœ‰é›†åˆæ˜¯å¦åŒ…å«ç‰ˆæœ¬å­—æ®µï¼Œä¸åŒ…å«åˆ™è‡ªåŠ¨æ·»åŠ ï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
            schema = self.collection.schema
            if not any(f.name == "embedding_model_version" for f in schema.fields):
                print("âš ï¸ ç°æœ‰Milvusé›†åˆç¼ºå°‘ç‰ˆæœ¬å­—æ®µï¼Œæ­£åœ¨æ·»åŠ ...")
                self.collection.add_field(
                    FieldSchema(name="embedding_model_version", dtype=DataType.VARCHAR, max_length=64,
                                default_value="unknown")
                )
            print(f"âœ… Milvusé›†åˆ[{self.collection_name}]å·²å­˜åœ¨ï¼Œç›´æ¥åŠ è½½")

        self.collection.load()

    def _load_all_versions(self) -> None:
        """åŠ è½½ç°æœ‰æ‰€æœ‰æ¨¡å‹ç‰ˆæœ¬ï¼Œç”¨äºå…¼å®¹æ€§æ£€æŸ¥"""
        self.available_versions = set()
        if not MILVUS_AVAILABLE or not self.collection:
            # å†…å­˜æ¨¡å¼ä¸‹ä»æœ¬åœ°æ•°æ®åŠ è½½ç‰ˆæœ¬
            self.available_versions = set(self.in_memory_versions) if hasattr(self, "in_memory_versions") else set()
            return

        # Milvusæ¨¡å¼ä¸‹æŸ¥è¯¢æ‰€æœ‰ç‰ˆæœ¬
        try:
            res = self.collection.query(
                expr="1==1",
                output_fields=["embedding_model_version"],
                limit=0  # ä¸é™æ•°é‡
            )
            self.available_versions = {item["embedding_model_version"] for item in res if
                                       item["embedding_model_version"]}
            print(f"âœ… åŠ è½½Milvusä¸­å¯ç”¨æ¨¡å‹ç‰ˆæœ¬ï¼š{sorted(self.available_versions)}")
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ç‰ˆæœ¬ä¿¡æ¯å¤±è´¥ï¼š{str(e)}ï¼Œå¯ç”¨ç‰ˆæœ¬å°†åŠ¨æ€æ¢æµ‹")
            self.available_versions = set()

    def add_chunks_with_version(self, chunks: List[Dict[str, Any]], embeddings: List[List[float]],
                                model_version: str) -> None:
        """å¸¦æ¨¡å‹ç‰ˆæœ¬çš„Chunkæ’å…¥"""
        if not model_version:
            raise ValueError("æ¨¡å‹ç‰ˆæœ¬ä¸èƒ½ä¸ºç©ºï¼Œè¯·æŒ‡å®šæœ‰æ•ˆçš„ç‰ˆæœ¬æ ‡è¯†")

        if not MILVUS_AVAILABLE or not self.collection:
            # å†…å­˜æ¨¡å¼ä¸‹å­˜å‚¨å¸¦ç‰ˆæœ¬çš„Chunk
            self.in_memory_chunks.extend(chunks)
            self.in_memory_embeddings.extend(embeddings)
            self.in_memory_versions.extend([model_version] * len(chunks))
            self.available_versions.add(model_version)
            print(f"âš ï¸ å†…å­˜æ¨¡å¼ï¼šæ·»åŠ  {len(chunks)} ä¸ªChunkï¼ˆç‰ˆæœ¬ï¼š{model_version}ï¼‰")
            return

        # Milvusæ¨¡å¼ä¸‹æ„å»ºå¸¦ç‰ˆæœ¬çš„æ•°æ®
        data = []
        for idx, (chunk, emb) in enumerate(zip(chunks, embeddings)):
            chunk_id = f"chunk_{uuid.uuid4().hex[:16]}"
            data.append([
                chunk_id,
                emb,
                chunk["content"][:4095],
                chunk["metadata"],
                model_version  # å†™å…¥æ¨¡å‹ç‰ˆæœ¬
            ])

        insert_result = self.collection.insert(data=data, using="default")
        self.collection.flush()
        self.available_versions.add(model_version)
        print(
            f"âœ… å‘Milvusæ’å…¥ {len(insert_result.primary_keys)} ä¸ªChunkï¼ˆç‰ˆæœ¬ï¼š{model_version}ï¼Œæ€»æ•°é‡ï¼š{self.collection.num_entities}ï¼‰")

    def search_with_version(self, query_embedding: List[float], model_version: str,
                            top_k: int = 3, nprobe: int = 32) -> Tuple[List[Dict[str, Any]], bool]:
        """æŒ‰æ¨¡å‹ç‰ˆæœ¬è¿‡æ»¤çš„æ£€ç´¢"""
        start_time = time.time()
        version_matched = True

        # æ£€æŸ¥ç‰ˆæœ¬æ˜¯å¦å­˜åœ¨
        if model_version not in self.available_versions:
            # åŠ¨æ€æ¢æµ‹ç‰ˆæœ¬æ˜¯å¦å­˜åœ¨ï¼ˆé¿å…åˆå§‹åŒ–æ—¶åŠ è½½å¤±è´¥ï¼‰
            if MILVUS_AVAILABLE and self.collection:
                try:
                    res = self.collection.query(
                        expr=f'embedding_model_version == "{model_version}"',
                        limit=1
                    )
                    if res:
                        self.available_versions.add(model_version)
                        version_matched = True
                    else:
                        version_matched = False
                except Exception as e:
                    print(f"âš ï¸ ç‰ˆæœ¬æ¢æµ‹å¤±è´¥ï¼š{str(e)}ï¼Œå°†è¿”å›å…¨ç‰ˆæœ¬ç»“æœ")
                    version_matched = False
            else:
                # å†…å­˜æ¨¡å¼ä¸‹æ£€æŸ¥ç‰ˆæœ¬
                version_matched = model_version in self.in_memory_versions

        if not version_matched:
            print(f"âš ï¸ æ¨¡å‹ç‰ˆæœ¬[{model_version}]åœ¨Milvusä¸­ä¸å­˜åœ¨ï¼Œå°†è¿”å›å…¨ç‰ˆæœ¬ç»“æœï¼ˆå¯èƒ½ä¸åŒ¹é…ï¼‰")
            expr = "1==1"  # åŒ¹é…æ‰€æœ‰ç‰ˆæœ¬
        else:
            expr = f'embedding_model_version == "{model_version}"'  # ä»…åŒ¹é…ç›®æ ‡ç‰ˆæœ¬

        # æ‰§è¡Œæ£€ç´¢
        if not MILVUS_AVAILABLE or not self.collection:
            # å†…å­˜æ¨¡å¼ä¸‹æŒ‰ç‰ˆæœ¬è¿‡æ»¤
            if version_matched:
                # è¿‡æ»¤å‡ºç›®æ ‡ç‰ˆæœ¬çš„ç´¢å¼•
                version_indices = [i for i, v in enumerate(self.in_memory_versions) if v == model_version]
                filtered_emb = [self.in_memory_embeddings[i] for i in version_indices]
                filtered_chunks = [self.in_memory_chunks[i] for i in version_indices]
            else:
                filtered_emb = self.in_memory_embeddings
                filtered_chunks = self.in_memory_chunks

            # å†…å­˜æ¨¡å¼ä¸‹å‘é‡æ£€ç´¢
            from numpy.linalg import norm
            import numpy as np
            if not filtered_emb:
                return [], version_matched
            emb_matrix = np.array(filtered_emb)
            query_emb = np.array(query_embedding)
            sims = emb_matrix @ query_emb / (norm(emb_matrix, axis=1) * norm(query_emb) + 1e-8)
            indices = sims.argsort()[::-1][:top_k]
            results = [
                {
                    "id": f"memory_chunk_{idx}",
                    "content": filtered_chunks[i]["content"],
                    "metadata": filtered_chunks[i]["metadata"],
                    "similarity": round(sims[i], 4),
                    "model_version": self.in_memory_versions[version_indices[i]] if version_matched else
                    self.in_memory_versions[i]
                }
                for i in indices
            ]
            print(
                f"âš ï¸ å†…å­˜æ¨¡å¼æ£€ç´¢å®Œæˆï¼ˆç‰ˆæœ¬åŒ¹é…ï¼š{version_matched}ï¼‰ï¼Œè€—æ—¶ï¼š{round((time.time() - start_time) * 1000, 2)}ms")
            return results, version_matched

        # Milvusæ¨¡å¼ä¸‹æŒ‰ç‰ˆæœ¬æ£€ç´¢
        search_params = {"metric_type": "IP", "params": {"nprobe": nprobe}}
        search_result = self.collection.search(
            data=[query_embedding],
            anns_field="embedding",
            param=search_params,
            limit=top_k,
            expr=expr,  # æŒ‰ç‰ˆæœ¬è¿‡æ»¤
            output_fields=["content", "metadata", "embedding_model_version"],
            using="default"
        )

        chunks = []
        for hits in search_result:
            for hit in hits:
                chunks.append({
                    "id": hit.id,
                    "content": hit.entity.get("content"),
                    "metadata": hit.entity.get("metadata"),
                    "similarity": round(hit.score, 4),
                    "model_version": hit.entity.get("embedding_model_version", "unknown")
                })

        search_duration = round((time.time() - start_time) * 1000, 2)
        print(f"âœ… Milvusæ£€ç´¢å®Œæˆï¼ˆç‰ˆæœ¬ï¼š{model_version}ï¼ŒåŒ¹é…ï¼š{version_matched}ï¼Œtop-{top_k}ï¼‰ï¼Œè€—æ—¶ï¼š{search_duration}ms")
        return chunks, version_matched

    def get_compatible_versions(self) -> List[str]:
        """è·å–æ‰€æœ‰å…¼å®¹çš„æ¨¡å‹ç‰ˆæœ¬"""
        return sorted(self.available_versions)


# ===================== 4. å…œåº•æ£€ç´¢å·¥å…·ï¼ˆå¢å¼ºBM25èåˆï¼‰ =====================
class KeywordFallbackRetriever:
    """å…œåº•æ£€ç´¢å™¨ï¼šç»“åˆTF-IDFå’ŒBM25ï¼Œå¢å¼ºå…³é”®è¯å¯†é›†å‹é—®é¢˜å¬å›"""

    def __init__(self, all_chunks: List[Dict[str, Any]] = None):
        self.all_chunks = all_chunks or []
        self.tfidf_vectorizer = None
        self.chunk_vectors = None
        self.bm25 = None  # æ–°å¢ï¼šBM25æ¨¡å‹
        self.bm25_tokenized_corpus = []  # æ–°å¢ï¼šBM25åˆ†è¯è¯­æ–™
        self._build_tfidf_index()  # æ„å»ºTF-IDFç´¢å¼•
        self._build_bm25_index()  # æ–°å¢ï¼šæ„å»ºBM25ç´¢å¼•
        self._build_rule_map()  # æ„å»ºè§„åˆ™åŒ¹é…æ˜ å°„

    def _build_bm25_index(self) -> None:
        """æ–°å¢ï¼šæ„å»ºBM25ç´¢å¼•ï¼Œä¼˜åŒ–å…³é”®è¯æ£€ç´¢"""
        if not self.all_chunks:
            print("âš ï¸ BM25ç´¢å¼•æ„å»ºæ¡ä»¶ä¸è¶³ï¼ˆç¼ºå°‘æ•°æ®ï¼‰")
            return

        try:
            # åˆ†è¯ï¼ˆç®€å•ç©ºæ ¼åˆ†å‰²ï¼Œå¯æ›¿æ¢ä¸ºæ›´å¤æ‚çš„åˆ†è¯å™¨ï¼‰
            self.bm25_tokenized_corpus = [chunk["content"].split() for chunk in self.all_chunks]
            self.bm25 = BM25Okapi(self.bm25_tokenized_corpus)
            print(f"âœ… BM25ç´¢å¼•æ„å»ºå®Œæˆï¼ˆ{len(self.all_chunks)}ä¸ªChunkï¼‰")
        except Exception as e:
            print(f"âš ï¸ BM25ç´¢å¼•æ„å»ºå¤±è´¥ï¼š{str(e)}")
            self.bm25 = None

    def _build_tfidf_index(self) -> None:
        """æ„å»ºTF-IDFç´¢å¼•ï¼Œç”¨äºå…³é”®è¯ç›¸ä¼¼åº¦åŒ¹é…"""
        if not TFIDF_AVAILABLE or not self.all_chunks:
            print("âš ï¸ TF-IDFç´¢å¼•æ„å»ºæ¡ä»¶ä¸è¶³ï¼ˆç¼ºå°‘ä¾èµ–æˆ–æ•°æ®ï¼‰")
            return

        try:
            # æå–æ‰€æœ‰Chunkçš„æ–‡æœ¬å†…å®¹ç”¨äºè®­ç»ƒ
            chunk_texts = [chunk["content"] for chunk in self.all_chunks]
            # åˆå§‹åŒ–TF-IDFå‘é‡å™¨ï¼ˆè¿‡æ»¤åœç”¨è¯ï¼Œé€‚é…ä¸­æ–‡ï¼‰
            self.tfidf_vectorizer = TfidfVectorizer(
                stop_words="english",  # å¯æ›¿æ¢ä¸ºä¸­æ–‡åœç”¨è¯è¡¨
                ngram_range=(1, 3),  # 1-3å…ƒè¯­æ³•ï¼Œå…¼é¡¾å•å­—å’ŒçŸ­è¯­
                max_features=10000
            )
            self.chunk_vectors = self.tfidf_vectorizer.fit_transform(chunk_texts)
            print(f"âœ… TF-IDFç´¢å¼•æ„å»ºå®Œæˆï¼ˆ{len(self.all_chunks)}ä¸ªChunkï¼Œ{len(self.tfidf_vectorizer.vocabulary_)}ä¸ªç‰¹å¾ï¼‰")
        except Exception as e:
            print(f"âš ï¸ TF-IDFç´¢å¼•æ„å»ºå¤±è´¥ï¼š{str(e)}")
            self.tfidf_vectorizer = None
            self.chunk_vectors = None

    def _build_rule_map(self) -> None:
        """æ„å»ºè§„åˆ™åŒ¹é…æ˜ å°„ï¼ˆé’ˆå¯¹é‡‘èåœºæ™¯ä¼˜åŒ–ï¼‰"""
        self.rule_map = {
            # é‡‘èå…³é”®è¯â†’åŒ¹é…è§„åˆ™
            "å‡€åˆ©æ¶¦": lambda c: "å‡€åˆ©æ¶¦" in c["content"] or "net profit" in c["content"].lower(),
            "è¥ä¸šæ”¶å…¥": lambda c: "è¥ä¸šæ”¶å…¥" in c["content"] or "revenue" in c["content"].lower(),
            "åŒæ¯”å¢é•¿ç‡": lambda c: "åŒæ¯”å¢é•¿" in c["content"] or "year-on-year" in c["content"].lower(),
            "èµ„äº§å‡å€¼æŸå¤±": lambda c: "èµ„äº§å‡å€¼" in c["content"] or "impairment" in c["content"].lower(),
            "æ¯›åˆ©ç‡": lambda c: "æ¯›åˆ©ç‡" in c["content"] or "gross margin" in c["content"].lower()
        }
        # æ•°å­—/æ—¥æœŸåŒ¹é…è§„åˆ™ï¼ˆé€šç”¨ï¼‰
        self.number_pattern = re.compile(r"\d+(\.\d+)?(%|å…ƒ|ä¸‡å…ƒ|äº¿å…ƒ|å¹´|æœˆ|æ—¥)")
        print("âœ… è§„åˆ™åŒ¹é…æ˜ å°„æ„å»ºå®Œæˆ")

    def update_chunks(self, chunks: List[Dict[str, Any]]) -> None:
        """æ›´æ–°Chunkæ•°æ®å¹¶é‡å»ºæ‰€æœ‰ç´¢å¼•"""
        self.all_chunks = chunks
        self._build_tfidf_index()
        self._build_bm25_index()  # æ–°å¢ï¼šæ›´æ–°BM25ç´¢å¼•
        self._build_rule_map()

    def bm25_search(self, query: str, top_k: int = 20) -> List[Dict[str, Any]]:
        """æ–°å¢ï¼šBM25å…³é”®è¯æ£€ç´¢"""
        if not self.bm25 or not self.all_chunks:
            return []
        
        tokenized_query = query.split()
        scores = self.bm25.get_scores(tokenized_query)
        top_indices = np.argsort(scores)[::-1][:top_k]
        return [self.all_chunks[i] for i in top_indices]

    def retrieve(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """å…œåº•æ£€ç´¢æ‰§è¡Œï¼šèåˆBM25+TF-IDF+è§„åˆ™"""
        start_time = time.time()
        if not self.all_chunks:
            print("âŒ å…œåº•æ£€ç´¢æ— å¯ç”¨Chunkæ•°æ®")
            return []

        # æ­¥éª¤1ï¼šBM25æ£€ç´¢ï¼ˆå…³é”®è¯å¯†é›†å‹é—®é¢˜ä¼˜å…ˆï¼‰
        bm25_results = self.bm25_search(query, top_k=top_k*2)  # å–æ›´å¤šå€™é€‰

        # æ­¥éª¤2ï¼šè§„åˆ™åŒ¹é…ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        rule_matched = []
        for chunk in bm25_results:  # åŸºäºBM25ç»“æœå†è¿‡æ»¤
            match_score = 0
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…é‡‘èå…³é”®è¯è§„åˆ™
            for keyword, rule_func in self.rule_map.items():
                if keyword in query and rule_func(chunk):
                    match_score += 2  # è§„åˆ™åŒ¹é…å¾—åˆ†æƒé‡é«˜
            # æ£€æŸ¥æ•°å­—/æ—¥æœŸåŒ¹é…ï¼ˆé‡‘èæ•°æ®å¸¸ç”¨ï¼‰
            query_numbers = set(self.number_pattern.findall(query))
            chunk_numbers = set(self.number_pattern.findall(chunk["content"]))
            if query_numbers & chunk_numbers:
                match_score += 1.5  # æ•°å­—åŒ¹é…å¾—åˆ†æ¬¡é«˜

            if match_score > 0:
                rule_matched.append((chunk, match_score))

        # æ­¥éª¤3ï¼šTF-IDFå…³é”®è¯ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆè¡¥å……è§„åˆ™æœªåŒ¹é…çš„ç»“æœï¼‰
        tfidf_matched = []
        if TFIDF_AVAILABLE and self.tfidf_vectorizer and self.chunk_vectors is not None:
            try:
                query_vector = self.tfidf_vectorizer.transform([query])
                similarities = cosine_similarity(query_vector, self.chunk_vectors)[0]
                # å–ç›¸ä¼¼åº¦å‰10çš„ç»“æœï¼ˆé¿å…ä¸è§„åˆ™åŒ¹é…é‡å¤ï¼‰
                tfidf_candidates = [
                    (self.all_chunks[i], float(similarities[i]))
                    for i in similarities.argsort()[::-1][:10]
                    if self.all_chunks[i] not in [c for c, _ in rule_matched]
                ]
                tfidf_matched = [(c, s * 1.0) for c, s in tfidf_candidates]  # ç›¸ä¼¼åº¦å¾—åˆ†æƒé‡ä¸­ç­‰
            except Exception as e:
                print(f"âš ï¸ TF-IDFåŒ¹é…å¤±è´¥ï¼š{str(e)}")

        # æ­¥éª¤4ï¼šç»¼åˆæ’åºï¼ˆè§„åˆ™åŒ¹é… > BM25 > TF-IDF > éšæœºï¼‰
        all_candidates = rule_matched + [(c, 1.0) for c in bm25_results if c not in [rc[0] for rc in rule_matched]] + tfidf_matched
        # è‹¥æ— åŒ¹é…ç»“æœï¼Œè¿”å›ä»»æ„top-kç»“æœï¼ˆé¿å…ç©ºè¿”å›ï¼‰
        if not all_candidates:
            all_candidates = [(chunk, 0.1) for chunk in self.all_chunks[:top_k]]

        # æŒ‰å¾—åˆ†æ’åºï¼Œç›¸åŒå¾—åˆ†æŒ‰Chunké•¿åº¦ï¼ˆä¼˜å…ˆçŸ­æ–‡æœ¬ï¼Œæ›´ç²¾å‡†ï¼‰
        all_candidates.sort(key=lambda x: (-x[1], len(x[0]["content"])))
        final_results = [
            {
                **c,
                "similarity": round(score, 4),
                "retrieval_type": "keyword_fallback",
                "model_version": "fallback"
            }
            for c, score in all_candidates[:top_k]
        ]

        print(f"âœ… å…œåº•å…³é”®è¯æ£€ç´¢å®Œæˆï¼ˆ{len(final_results)}ä¸ªç»“æœï¼‰ï¼Œè€—æ—¶ï¼š{round((time.time() - start_time) * 1000, 2)}ms")
        return final_results


# ===================== 5. æµé‡åˆ†å±‚è·¯ç”±å·¥å…·ï¼ˆä¿ç•™åŸæœ‰åŠŸèƒ½ï¼‰ =====================
class LLMClientRouter:
    def __init__(self):
        self.local_config = {
            "api_key": os.getenv("LOCAL_LLM_API_KEY", os.getenv("LOCAL_API_KEY")),
            "base_url": os.getenv("LOCAL_LLM_BASE_URL", os.getenv("LOCAL_BASE_URL")),
            "model": os.getenv("LOCAL_LLM_MODEL", os.getenv("LOCAL_TEXT_MODEL", "qwen2.5-7b-instruct")),
            "desc": "æœ¬åœ°A6000 GPU"
        }
        self.cloud_config = {
            "api_key": os.getenv("CLOUD_LLM_API_KEY"),
            "base_url": os.getenv("CLOUD_LLM_BASE_URL"),
            "model": os.getenv("CLOUD_LLM_MODEL", "qwen2.5-72b-instruct"),
            "desc": "äº‘ç«¯å¤§æ¨¡å‹æœåŠ¡"
        }

        self._validate_config()
        self.off_peak_hours = (22, 8)
        print(f"âœ… LLMè·¯ç”±åˆå§‹åŒ–å®Œæˆï¼š")
        print(
            f"   - éå³°å€¼æ—¶æ®µï¼ˆ{self.off_peak_hours[0]}:00-{self.off_peak_hours[1]}:00ï¼‰ä½¿ç”¨ï¼š{self.local_config['desc']}")
        print(f"   - å³°å€¼æ—¶æ®µï¼ˆ{self.off_peak_hours[1]}:00-{self.off_peak_hours[0]}:00ï¼‰ä½¿ç”¨ï¼š{self.cloud_config['desc']}")

    def _validate_config(self) -> None:
        if not (self.local_config["api_key"] and self.local_config["base_url"]):
            raise ValueError("æœ¬åœ°LLMé…ç½®ä¸å®Œæ•´ï¼Œè¯·åœ¨.envä¸­è®¾ç½®LOCAL_LLM_API_KEYå’ŒLOCAL_LLM_BASE_URL")
        if not (self.cloud_config["api_key"] and self.cloud_config["base_url"]):
            raise ValueError("äº‘ç«¯LLMé…ç½®ä¸å®Œæ•´ï¼Œè¯·åœ¨.envä¸­è®¾ç½®CLOUD_LLM_API_KEYå’ŒCLOUD_LLM_BASE_URL")

    def _is_off_peak(self) -> bool:
        current_hour = time.localtime().tm_hour
        start_off_peak, end_off_peak = self.off_peak_hours
        return current_hour >= start_off_peak or current_hour < end_off_peak

    def get_client(self) -> Tuple[OpenAI, Dict[str, str]]:
        if self._is_off_peak():
            client = OpenAI(
                api_key=self.local_config["api_key"],
                base_url=self.local_config["base_url"]
            )
            meta = {
                "client_type": "local",
                "desc": self.local_config["desc"],
                "model": self.local_config["model"],
                "hour": time.localtime().tm_hour,
                "is_off_peak": True
            }
        else:
            client = OpenAI(
                api_key=self.cloud_config["api_key"],
                base_url=self.cloud_config["base_url"]
            )
            meta = {
                "client_type": "cloud",
                "desc": self.cloud_config["desc"],
                "model": self.cloud_config["model"],
                "hour": time.localtime().tm_hour,
                "is_off_peak": False
            }

        print(f"ğŸ”€ LLMè·¯ç”±ï¼šå½“å‰{meta['hour']}ç‚¹ï¼Œä½¿ç”¨{meta['client_type']}å®¢æˆ·ç«¯ï¼ˆ{meta['desc']}ï¼‰")
        return client, meta

    def get_model(self) -> str:
        return self.local_config["model"] if self._is_off_peak() else self.cloud_config["model"]


# ===================== 6. åµŒå…¥æ¨¡å‹ï¼ˆå¢å¼ºç‰ˆæœ¬ç®¡ç†ï¼‰ =====================
class VersionedEmbeddingModel:
    """å¢å¼ºç‰ˆåµŒå…¥æ¨¡å‹ï¼šæ”¯æŒç‰ˆæœ¬ç®¡ç†å’Œå›æ»šåŠ è½½"""

    def __init__(self, batch_size: int = 64):
        # ä»ç¯å¢ƒå˜é‡è¯»å–å½“å‰ä½¿ç”¨çš„æ¨¡å‹ç‰ˆæœ¬å’Œé…ç½®
        self.current_version = os.getenv("EMBEDDING_MODEL_VERSION", "bge-m3-v202405")
        self.model_configs = self._load_version_configs()  # åŠ è½½æ‰€æœ‰ç‰ˆæœ¬çš„æ¨¡å‹é…ç½®
        self.batch_size = batch_size

        # éªŒè¯å½“å‰ç‰ˆæœ¬æ˜¯å¦å­˜åœ¨
        if self.current_version not in self.model_configs:
            print(f"âš ï¸ å½“å‰é…ç½®çš„æ¨¡å‹ç‰ˆæœ¬[{self.current_version}]ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤ç‰ˆæœ¬")
            self.current_version = next(iter(self.model_configs.keys())) if self.model_configs else "unknown"

        # åŠ è½½å½“å‰ç‰ˆæœ¬çš„æ¨¡å‹
        self._load_model(self.current_version)
        print(f"âœ… åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼ˆç‰ˆæœ¬ï¼š{self.current_version}ï¼Œæ¨¡å‹ï¼š{self.embedding_model}ï¼‰")

    def _load_version_configs(self) -> Dict[str, Dict[str, str]]:
        """åŠ è½½æ‰€æœ‰æ”¯æŒçš„æ¨¡å‹ç‰ˆæœ¬é…ç½®"""
        # ä»ç¯å¢ƒå˜é‡åŠ è½½å¤šç‰ˆæœ¬é…ç½®ï¼ˆæ”¯æŒå¤šç‰ˆæœ¬å¹¶å­˜ï¼‰
        version_configs = {}
        # åŸºç¡€ç‰ˆæœ¬é…ç½®ï¼ˆé»˜è®¤ï¼‰
        base_config = {
            "api_key": os.getenv("EMBEDDING_API_KEY", os.getenv("LOCAL_API_KEY")),
            "base_url": os.getenv("EMBEDDING_BASE_URL", os.getenv("LOCAL_BASE_URL")),
            "model_name": os.getenv("EMBEDDING_MODEL", "BAAI/bge-m3")
        }
        version_configs["bge-m3-v202405"] = base_config

        # å…¶ä»–ç‰ˆæœ¬é…ç½®ï¼ˆå¯æ‰©å±•ï¼‰
        extra_versions = os.getenv("EXTRA_EMBEDDING_VERSIONS", "").split(",") if os.getenv(
            "EXTRA_EMBEDDING_VERSIONS") else []
        for ver in extra_versions:
            ver = ver.strip()
            if not ver:
                continue
            version_configs[ver] = {
                "api_key": os.getenv(f"EMBEDDING_API_KEY_{ver.upper()}", base_config["api_key"]),
                "base_url": os.getenv(f"EMBEDDING_BASE_URL_{ver.upper()}", base_config["base_url"]),
                "model_name": os.getenv(f"EMBEDDING_MODEL_{ver.upper()}", base_config["model_name"])
            }

        print(f"âœ… åŠ è½½åµŒå…¥æ¨¡å‹ç‰ˆæœ¬é…ç½®ï¼š{sorted(version_configs.keys())}")
        return version_configs

    def _load_model(self, version: str) -> None:
        """æ ¹æ®ç‰ˆæœ¬åŠ è½½æŒ‡å®šæ¨¡å‹"""
        if version not in self.model_configs:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç‰ˆæœ¬ï¼š{version}ï¼Œå¯ç”¨ç‰ˆæœ¬ï¼š{sorted(self.model_configs.keys())}")

        config = self.model_configs[version]
        self.api_key = config["api_key"]
        self.base_url = config["base_url"]
        self.embedding_model = config["model_name"]

        # éªŒè¯é…ç½®å®Œæ•´æ€§
        if not self.api_key or not self.base_url:
            raise ValueError(f"æ¨¡å‹ç‰ˆæœ¬[{version}]é…ç½®ä¸å®Œæ•´ï¼Œç¼ºå°‘api_keyæˆ–base_url")

    def switch_version(self, target_version: str) -> bool:
        """åˆ‡æ¢æ¨¡å‹ç‰ˆæœ¬ï¼ˆç”¨äºå›æ»šåœºæ™¯ï¼‰"""
        if target_version == self.current_version:
            print(f"â„¹ï¸ å½“å‰å·²ä½¿ç”¨æ¨¡å‹ç‰ˆæœ¬[{target_version}]ï¼Œæ— éœ€åˆ‡æ¢")
            return True
        if target_version not in self.model_configs:
            print(f"âŒ æ— æ³•åˆ‡æ¢åˆ°ç‰ˆæœ¬[{target_version}]ï¼Œå¯ç”¨ç‰ˆæœ¬ï¼š{sorted(self.model_configs.keys())}")
            return False

        try:
            self._load_model(target_version)
            self.current_version = target_version
            print(f"âœ… æˆåŠŸåˆ‡æ¢åµŒå…¥æ¨¡å‹ç‰ˆæœ¬ï¼š{target_version}ï¼ˆæ¨¡å‹ï¼š{self.embedding_model}ï¼‰")
            return True
        except Exception as e:
            print(f"âŒ åˆ‡æ¢ç‰ˆæœ¬[{target_version}]å¤±è´¥ï¼š{str(e)}ï¼Œä¿æŒå½“å‰ç‰ˆæœ¬[{self.current_version}]")
            return False

    def get_available_versions(self) -> List[str]:
        """è·å–æ‰€æœ‰æ”¯æŒçš„æ¨¡å‹ç‰ˆæœ¬"""
        return sorted(self.model_configs.keys())

    def embed_texts(self, texts: List[str]) -> List[List[float]]:
        return get_text_embedding(
            texts,
            api_key=self.api_key,
            base_url=self.base_url,
            embedding_model=self.embedding_model,
            batch_size=self.batch_size
        )

    def embed_text(self, text: str) -> List[float]:
        return self.embed_texts([text])[0]


# ===================== 7. æ ¸å¿ƒRAGç±»ï¼ˆæ•´åˆå¤šè·¯å¬å›ï¼‰ =====================
class SimpleRAG:
    def __init__(self, chunk_json_path: str, batch_size: int = 32,
                 max_chunk_tokens: int = 512, slide_step: int = 200,
                 milvus_dim: int = 1536):
        self.enable_finance_mode = os.getenv('ENABLE_FINANCE_MODE', 'false').lower() == 'true'
        self.enable_fp8 = os.getenv('ENABLE_FP8_INFERENCE', 'false').lower() == 'true'
        self.max_chunk_tokens = max_chunk_tokens
        self.slide_step = slide_step

        # 1. åˆå§‹åŒ–å®¡è®¡æ—¥å¿—ï¼ˆå¢å¼ºç‰ˆæœ¬è®°å½•ï¼‰
        self.audit_logger = AuditLogManager(
            redis_host=os.getenv("REDIS_HOST", "localhost"),
            redis_port=int(os.getenv("REDIS_PORT", 6379)),
            redis_db=int(os.getenv("REDIS_DB", 0))
        )

        # 2. åˆå§‹åŒ–ChunkåŠ è½½å™¨
        self.loader = PageChunkLoader(chunk_json_path)
        self.raw_chunks = self.loader.load_chunks()  # åŠ è½½åŸå§‹Chunkç”¨äºå…œåº•æ£€ç´¢

        # 3. åˆå§‹åŒ–ç‰ˆæœ¬åŒ–åµŒå…¥æ¨¡å‹ï¼ˆæ ¸å¿ƒå¢å¼ºï¼‰
        self.embedding_model = VersionedEmbeddingModel(batch_size=batch_size)
        self.current_embedding_version = self.embedding_model.current_version

        # 4. åˆå§‹åŒ–å¸¦ç‰ˆæœ¬çš„Milvuså‘é‡åº“ï¼ˆæ ¸å¿ƒå¢å¼ºï¼‰
        self.vector_store = MilvusVectorStore(
            collection_name=os.getenv("MILVUS_COLLECTION", "rag_finance_chunks"),
            dim=milvus_dim
        )
        self.vector_store.set_embedding_version(self.current_embedding_version)  # åŒæ­¥ç‰ˆæœ¬åˆ°å‘é‡åº“

        # 5. åˆå§‹åŒ–BGEé‡æ’æ¨¡å‹ï¼ˆæ ¸å¿ƒæ–°å¢ï¼‰
        self.reranker = FlagReranker(
            'BAAI/bge-reranker-large',
            use_fp16=True,
            devices=["cuda:0"] if torch.cuda.is_available() else ["cpu"]
        )
        print("âœ… BGEé‡æ’æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

        # 6. åˆå§‹åŒ–LLMæµé‡è·¯ç”±
        self.llm_router = LLMClientRouter()

        # 7. åˆå§‹åŒ–å…œåº•æ£€ç´¢å™¨ï¼ˆå«BM25ï¼‰
        self.fallback_retriever = KeywordFallbackRetriever()

        # 8. é‡‘èæ¨¡å¼æ‰©å±•
        if self.enable_finance_mode:
            try:
                from finance_rag_extension import FinanceVectorStore
                self.vector_store = FinanceVectorStore()
                print("âœ… å¯ç”¨é‡‘èç‰ˆå‘é‡å­˜å‚¨")
            except ImportError:
                print("âš ï¸ æœªæ‰¾åˆ°finance_rag_extensionï¼Œä½¿ç”¨é»˜è®¤Milvuså‘é‡å­˜å‚¨")

    def setup(self):
        """åŠ è½½Chunkã€ç”ŸæˆåµŒå…¥ã€å†™å…¥å‘é‡åº“ï¼ˆå¢å¼ºç‰ˆæœ¬ç»‘å®šï¼‰"""
        print("=" * 50)
        print("å¼€å§‹RAGç³»ç»Ÿåˆå§‹åŒ–...")

        # 1. å¤„ç†è¶…é•¿Chunk
        print("1. å¤„ç†è¶…é•¿Chunkï¼ˆæ»‘åŠ¨çª—å£+æ•°å­—ä¼˜å…ˆæˆªæ–­ï¼‰...")
        processed_chunks = []
        for raw_chunk in tqdm(self.raw_chunks, desc="   Chunké¢„å¤„ç†"):
            sub_chunks = process_long_chunk(
                chunk=raw_chunk,
                max_tokens=self.max_chunk_tokens,
                slide_step=self.slide_step
            )
            processed_chunks.extend(sub_chunks)
        print(f"   é¢„å¤„ç†åæ€»Chunkæ•°: {len(processed_chunks)}ï¼ˆå«å­Chunkï¼‰")

        # 2. æ›´æ–°å…œåº•æ£€ç´¢å™¨çš„Chunkæ•°æ®ï¼ˆå«BM25ç´¢å¼•ï¼‰
        self.fallback_retriever.update_chunks(processed_chunks)

        # 3. ç”Ÿæˆå‘é‡åµŒå…¥ï¼ˆå¸¦ç‰ˆæœ¬ï¼‰
        print(f"2. ç”Ÿæˆæ–‡æœ¬åµŒå…¥ï¼ˆç‰ˆæœ¬ï¼š{self.current_embedding_version}ï¼‰...")
        start_embed = time.time()
        embeddings = self.embedding_model.embed_texts([c['content'] for c in processed_chunks])
        embed_duration = round((time.time() - start_embed) * 1000, 2)
        print(f"   åµŒå…¥ç”Ÿæˆå®Œæˆï¼ˆ{len(embeddings)}ä¸ªå‘é‡ï¼‰ï¼Œè€—æ—¶ï¼š{embed_duration}ms")

        # 4. å¸¦ç‰ˆæœ¬å†™å…¥å‘é‡åº“ï¼ˆæ ¸å¿ƒå¢å¼ºï¼‰
        print("3. å‘å‘é‡åº“å†™å…¥å¸¦ç‰ˆæœ¬çš„Chunk...")
        self.vector_store.add_chunks_with_version(
            chunks=processed_chunks,
            embeddings=embeddings,
            model_version=self.current_embedding_version
        )

        print("âœ… RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
        print(f"   - å‘é‡åº“ç±»å‹ï¼š{'Milvus' if MILVUS_AVAILABLE and self.vector_store.collection else 'å†…å­˜æ¨¡å¼'}")
        print(
            f"   - åµŒå…¥æ¨¡å‹ç‰ˆæœ¬ï¼š{self.current_embedding_version}ï¼ˆæ”¯æŒç‰ˆæœ¬ï¼š{self.embedding_model.get_available_versions()}ï¼‰")
        print(f"   - é‡æ’åºæ¨¡å‹ï¼šBAAI/bge-reranker-largeï¼ˆ{'GPU' if torch.cuda.is_available() else 'CPU'}æ¨¡å¼ï¼‰")
        print(f"   - å…œåº•æ£€ç´¢ï¼š{'å¯ç”¨' if TFIDF_AVAILABLE else 'ç¦ç”¨'}ï¼ˆå«BM25ï¼‰")
        print(f"   - é‡‘èæ¨¡å¼ï¼š{'å¯ç”¨' if self.enable_finance_mode else 'ç¦ç”¨'}")
        print("=" * 50)

    def switch_embedding_version(self, target_version: str) -> bool:
        """å¯¹å¤–æä¾›çš„ç‰ˆæœ¬åˆ‡æ¢æ¥å£ï¼ˆç”¨äºå›æ»šåœºæ™¯ï¼‰"""
        # åˆ‡æ¢åµŒå…¥æ¨¡å‹ç‰ˆæœ¬
        embed_switch_ok = self.embedding_model.switch_version(target_version)
        if embed_switch_ok:
            self.current_embedding_version = target_version
            self.vector_store.set_embedding_version(target_version)  # åŒæ­¥æ›´æ–°å‘é‡åº“ç‰ˆæœ¬
            # åŒæ­¥æ›´æ–°å‘é‡åº“çš„å¯ç”¨ç‰ˆæœ¬æ£€æŸ¥
            self.vector_store._load_all_versions()
            print(
                f"âœ… ç‰ˆæœ¬åˆ‡æ¢å®Œæˆï¼Œå½“å‰åµŒå…¥ç‰ˆæœ¬ï¼š{self.current_embedding_version}ï¼Œå‘é‡åº“å¯ç”¨ç‰ˆæœ¬ï¼š{sorted(self.vector_store.available_versions)}")
        return embed_switch_ok

    def retrieve_and_rerank(self, query: str, top_k: int = 3, candidate_top_k: int = 20) -> Tuple[
        List[Dict[str, Any]], str, bool]:
        """æ•´åˆæ£€ç´¢ä¸é‡æ’çš„æ ¸å¿ƒæ–¹æ³•ï¼šå‘é‡+BM25å¤šè·¯å¬å›"""
        start_total = time.time()
        print("\n" + "=" * 50)
        print(f"å¼€å§‹å¤„ç†æŸ¥è¯¢ï¼š{query[:50]}...")

        # æ­¥éª¤1ï¼šç”ŸæˆæŸ¥è¯¢å‘é‡ï¼ˆå½“å‰ç‰ˆæœ¬ï¼‰
        print(f"1. ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼ˆç‰ˆæœ¬ï¼š{self.current_embedding_version}ï¼‰...")
        start_embed = time.time()
        q_emb = self.embedding_model.embed_text(query)
        embed_duration = round((time.time() - start_embed) * 1000, 2)
        print(f"   æŸ¥è¯¢å‘é‡ç”Ÿæˆè€—æ—¶ï¼š{embed_duration}ms")

        # æ­¥éª¤2ï¼šå‘é‡æ£€ç´¢ï¼ˆç²—å¬å›ï¼‰
        print(f"2. æŒ‰ç‰ˆæœ¬[{self.current_embedding_version}]æ‰§è¡Œå‘é‡æ£€ç´¢ï¼ˆç²—å¬å›top-{candidate_top_k}ï¼‰...")
        vector_results, version_matched = self.vector_store.search_with_version(
            query_embedding=q_emb,
            model_version=self.current_embedding_version,
            top_k=candidate_top_k,
            nprobe=32
        )

        # æ­¥éª¤3ï¼šBM25å…³é”®è¯æ£€ç´¢ï¼ˆå¤šè·¯å¬å›ï¼‰
        print(f"3. æ‰§è¡ŒBM25å…³é”®è¯æ£€ç´¢ï¼ˆç²—å¬å›top-{candidate_top_k}ï¼‰...")
        bm25_results = self.fallback_retriever.bm25_search(query, top_k=candidate_top_k)

        # æ­¥éª¤4ï¼šèåˆå‘é‡å’ŒBM25ç»“æœï¼ˆå»é‡ï¼‰
        vector_ids = {res["id"] for res in vector_results}
        combined_results = vector_results + [res for res in bm25_results if res["id"] not in vector_ids]
        final_candidates = combined_results[:candidate_top_k]
        retrieval_type = "vector+bm25"

        # æ­¥éª¤5ï¼šBGEé‡æ’æ¨¡å‹ç²¾æ’åº
        print(f"4. ä½¿ç”¨BGEé‡æ’æ¨¡å‹å¯¹{len(final_candidates)}ä¸ªå€™é€‰ç»“æœç²¾æ’åº...")
        start_rerank = time.time()
        # æ„é€ ï¼ˆquery, chunkå†…å®¹ï¼‰å¯¹
        pairs = [(query, chunk["content"]) for chunk in final_candidates]
        # è®¡ç®—é‡æ’åˆ†æ•°
        scores = self.reranker.compute_score(pairs)
        # æŒ‰åˆ†æ•°æ’åºå¹¶å–top_k
        reranked_chunks = [
            chunk for _, chunk in sorted(zip(scores, final_candidates), key=lambda x: x[0], reverse=True)
        ][:top_k]
        rerank_duration = round((time.time() - start_rerank) * 1000, 2)
        print(f"   é‡æ’å®Œæˆï¼Œè€—æ—¶ï¼š{rerank_duration}ms")

        total_duration = round((time.time() - start_total) * 1000, 2)
        print(f"âœ… æŸ¥è¯¢å®Œæˆï¼ˆæ£€ç´¢ç±»å‹ï¼š{retrieval_type}ï¼Œç‰ˆæœ¬åŒ¹é…ï¼š{version_matched}ï¼‰ï¼Œæ€»è€—æ—¶ï¼š{total_duration}ms")
        print("=" * 50)

        return reranked_chunks, retrieval_type, version_matched

    def generate_answer(self, question: str, top_k: int = 3, user_id: Optional[str] = None) -> Dict[str, Any]:
        """ç”Ÿæˆå›ç­”ï¼ˆå¢å¼ºç‰ˆæœ¬è®°å½•å’Œå…œåº•æç¤ºï¼‰"""
        # 1. ç”Ÿæˆå®¡è®¡æ—¥å¿—ï¼ˆå«ç‰ˆæœ¬ä¿¡æ¯ï¼‰
        current_client_meta = self.llm_router.get_client()[1]
        log_metadata = {
            "user_id": user_id,
            "top_k": top_k,
            "llm_client_type": current_client_meta["client_type"],
            "llm_model": current_client_meta["model"],
            "embedding_model_version": self.current_embedding_version,
            "vector_store_type": "Milvus" if MILVUS_AVAILABLE and self.vector_store.collection else "Memory"
        }
        log_id = self.audit_logger.create_pre_log(question, log_metadata)
        print(f"ğŸ“Œ ç”Ÿæˆå®¡è®¡æ—¥å¿—ID: {log_id}")

        try:
            # 2. ä½¿ç”¨æ–°çš„æ£€ç´¢+é‡æ’æµç¨‹ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼‰
            retrieved_chunks, retrieval_type, version_matched = self.retrieve_and_rerank(
                question, top_k=top_k, candidate_top_k=20
            )

            # 3. åˆå¹¶ä¸Šä¸‹æ–‡
            merged_context = []
            processed_original_ids = set()
            for chunk in retrieved_chunks:
                meta = chunk["metadata"]
                original_id = f"{meta['file_name']}_{meta.get('page', 'unknown')}"
                if original_id not in processed_original_ids:
                    sub_chunk_info = (
                        f"[æ–‡ä»¶å]{meta['file_name']} [é¡µç ]{meta.get('page', 'unknown')} "
                        f"[å­Chunk {meta.get('sub_chunk_idx', 0)}/{meta.get('total_sub_chunks', 1)}] "
                        f"[æ£€ç´¢ç±»å‹:{chunk.get('retrieval_type', 'vector')}] "
                        f"[ç›¸ä¼¼åº¦{chunk.get('similarity', 0):.3f}]\n"
                        f"{chunk['content']}"
                    )
                    merged_context.append(sub_chunk_info)
                    processed_original_ids.add(original_id)
            context = "\n\n".join(merged_context)

            # 4. æ„å»ºPromptï¼ˆå¢å¼ºå…œåº•åœºæ™¯æç¤ºï¼‰
            prompt_content = ""
            if self.enable_finance_mode:
                prompt_content = (
                    f"ä½ æ˜¯èµ„æ·±é‡‘èåˆ†æå¸ˆï¼Œéœ€ä»æ£€ç´¢å†…å®¹ä¸­æå–å…³é”®ä¿¡æ¯å¹¶ä¸¥æ ¼éµå¾ªé‡‘èè§„èŒƒï¼š\n"
                    f"1. è‹¥æ¶‰åŠæ•°æ®ï¼Œç”¨è¡¨æ ¼å±•ç¤ºï¼ˆæŒ‡æ ‡|æ•°å€¼|æ¥æºé¡µç /å­Chunkï¼‰ï¼›\n"
                    f"2. è‹¥æ¶‰åŠæœ¯è¯­ï¼Œå…ˆè§£é‡Šå®šä¹‰å†åˆ†æå¸‚åœºå½±å“ï¼›\n"
                    f"3. æ‰€æœ‰ç»“è®ºéœ€æ³¨æ˜ä¾æ®ï¼ˆå¦‚\"æ ¹æ®XXç ”æŠ¥ç¬¬5é¡µå­Chunk 0/2æ•°æ®\"ï¼‰ï¼›\n"
                    f"4. ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼š{{\"answer\": \"ä½ çš„å›ç­”\", \"filename\": \"æ¥æºæ–‡ä»¶å\", \"page\": \"æ¥æºé¡µç \", \"note\": \"é™„åŠ è¯´æ˜\"}}\n"
                )
            else:
                prompt_content = (
                    f"ä½ æ˜¯ä¸€åä¸“ä¸šçš„åˆ†æåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ä»¥ä¸‹æ£€ç´¢åˆ°çš„å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚\n"
                    f"è¯·ä¸¥æ ¼æŒ‰ç…§å¦‚ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š\n"
                    f'{{"answer": "ä½ çš„ç®€æ´å›ç­”", "filename": "æ¥æºæ–‡ä»¶å", "page": "æ¥æºé¡µç ", "note": "é™„åŠ è¯´æ˜"}}\n'
                )

            # è‹¥ä½¿ç”¨å…œåº•æ£€ç´¢ï¼Œæ·»åŠ ç³»ç»Ÿæç¤º
            if "fallback" in retrieval_type:
                prompt_content += f"âš ï¸ ç³»ç»Ÿæç¤ºï¼šå½“å‰å¤„äºå…¼å®¹æ¨¡å¼ï¼ˆæ¨¡å‹ç‰ˆæœ¬åŒ¹é…ä¸­ï¼‰ï¼Œç»“æœåŸºäºå…³é”®è¯æ£€ç´¢ï¼Œå¯èƒ½å­˜åœ¨å»¶è¿Ÿï¼Œå»ºè®®ç¨åé‡è¯•ã€‚\n"

            prompt_content += f"æ£€ç´¢å†…å®¹ï¼š\n{context}\n\né—®é¢˜ï¼š{question}\n"
            prompt_content += f"è¯·ç¡®ä¿è¾“å‡ºä¸ºåˆæ³•JSONå­—ç¬¦ä¸²ï¼Œä¸è¦åŒ…å«å¤šä½™å†…å®¹ã€‚"

            # 5. è°ƒç”¨LLMç”Ÿæˆå›ç­”
            print("5. è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”...")
            start_llm = time.time()
            llm_client, llm_meta = self.llm_router.get_client()
            request_kwargs = {
                "model": llm_meta["model"],
                "messages": [
                    {"role": "system",
                     "content": "ä½ æ˜¯ä¸“ä¸šé‡‘èåˆ†æå¸ˆ" if self.enable_finance_mode else "ä½ æ˜¯ä¸“ä¸šåˆ†æåŠ©æ‰‹"},
                    {"role": "user", "content": prompt_content}
                ],
                "temperature": 0.2,
                "max_tokens": 1024,
                "stream": False
            }
            if self.enable_fp8 and llm_meta["client_type"] == "local":
                request_kwargs["extra_body"] = {"precision": "fp8"}
                print(f"   å¯ç”¨FP8ç²¾åº¦æ¨ç†ï¼ˆæœ¬åœ°GPUæ¨¡å¼ï¼‰")

            completion = llm_client.chat.completions.create(** request_kwargs)
            llm_duration = round((time.time() - start_llm) * 1000, 2)
            print(f"   å¤§æ¨¡å‹æ¨ç†è€—æ—¶ï¼š{llm_duration}msï¼ˆ{llm_meta['desc']}ï¼‰")

            # 6. è§£æå›ç­”ç»“æœ
            raw = completion.choices[0].message.content.strip()
            import json as pyjson
            from extract_json_array import extract_json_array
            json_str = extract_json_array(raw, mode='objects')
            answer = raw
            filename = ""
            page = ""
            note = ""
            if json_str:
                try:
                    arr = pyjson.loads(json_str)
                    if isinstance(arr, list) and arr:
                        j = arr[0]
                        answer = j.get('answer', raw)
                        filename = j.get('filename', '')
                        page = j.get('page', '')
                        note = j.get('note', f"æ£€ç´¢ç±»å‹ï¼š{retrieval_type}ï¼Œæ¨¡å‹ç‰ˆæœ¬ï¼š{self.current_embedding_version}")
                    else:
                        note = f"JSONè§£æå¼‚å¸¸ï¼Œæ£€ç´¢ç±»å‹ï¼š{retrieval_type}"
                except pyjson.JSONDecodeError:
                    note = f"JSONè§£æå¤±è´¥ï¼Œæ£€ç´¢ç±»å‹ï¼š{retrieval_type}"
            else:
                note = f"æœªæå–åˆ°JSONç»“æœï¼Œæ£€ç´¢ç±»å‹ï¼š{retrieval_type}"

            # 7. æ›´æ–°å®¡è®¡æ—¥å¿—ï¼ˆå«ç‰ˆæœ¬å’Œæ£€ç´¢ç±»å‹ï¼‰
            self.audit_logger.update_log_with_answer(
                log_id=log_id,
                answer=answer,
                retrieval_chunks=retrieved_chunks,
                status="completed",
                model_version=self.current_embedding_version,
                retrieval_type=retrieval_type
            )
            asyncio.create_task(self.audit_logger.write_audit_log_to_db(log_id))  # å¸¦é‡è¯•çš„å¼‚æ­¥è½ç›˜

            # 8. è¿”å›æœ€ç»ˆç»“æœï¼ˆå«ç‰ˆæœ¬å’Œå…œåº•ä¿¡æ¯ï¼‰
            total_duration = sum([
                # ä»æ£€ç´¢ç»“æœä¸­æå–è€—æ—¶ï¼ˆéœ€åœ¨query_with_fallbackä¸­è®°å½•ï¼‰
                chunk.get('retrieval_duration_ms', 0) for chunk in retrieved_chunks[:1]
            ]) + llm_duration
            return {
                "question": question,
                "answer": answer,
                "filename": filename,
                "page": page,
                "note": note,  # åŒ…å«æ£€ç´¢ç±»å‹å’Œç‰ˆæœ¬ä¿¡æ¯
                "retrieval_chunks": retrieved_chunks,
                "merged_context": merged_context,
                "audit_log_id": log_id,
                "llm_route_info": llm_meta,
                "version_info": {  # ç‰ˆæœ¬ä¿¡æ¯
                    "embedding_version": self.current_embedding_version,
                    "version_matched": version_matched,
                    "available_versions": sorted(self.vector_store.available_versions)
                },
                "performance_stats": {
                    "retrieval_duration_ms": round(total_duration - llm_duration, 2),
                    "llm_duration_ms": llm_duration,
                    "total_duration_ms": round(total_duration, 2)
                }
            }

        except Exception as e:
            error_msg = f"ç”Ÿæˆå›ç­”å¤±è´¥: {str(e)}"
            print(error_msg)
            # è®°å½•å¤±è´¥æ—¥å¿—ï¼ˆå«ç‰ˆæœ¬ä¿¡æ¯ï¼‰
            self.audit_logger.update_log_with_answer(
                log_id=log_id,
                answer=error_msg,
                retrieval_chunks=[],
                status="failed",
                model_version=self.current_embedding_version,
                retrieval_type="error"
            )
            asyncio.create_task(self.audit_logger.write_audit_log_to_db(log_id))  # å¸¦é‡è¯•çš„å¼‚æ­¥è½ç›˜
            return {
                "question": question,
                "answer": error_msg,
                "filename": "",
                "page": "",
                "note": f"ç³»ç»Ÿé”™è¯¯ï¼Œæ¨¡å‹ç‰ˆæœ¬ï¼š{self.current_embedding_version}",
                "retrieval_chunks": [],
                "merged_context": [],
                "audit_log_id": log_id,
                "llm_route_info": current_client_meta,
                "version_info": {
                    "embedding_version": self.current_embedding_version,
                    "version_matched": False,
                    "available_versions": sorted(self.vector_store.available_versions)
                },
                "error": str(e)
            }


# ===================== 8. åŸæœ‰è¾…åŠ©ç±»ï¼ˆä¿ç•™ï¼‰ =====================
class PageChunkLoader:
    def __init__(self, json_path: str):
        self.json_path = json_path

    def load_chunks(self) -> List[Dict[str, Any]]:
        with open(self.json_path, 'r', encoding='utf-8') as f:
            return json.load(f)


# ===================== 9. ä¸»å‡½æ•°ï¼ˆå¯ç”¨å…¨é‡æµ‹è¯•æ•°æ®ï¼‰ =====================
if __name__ == '__main__':
    # é…ç½®å‚æ•°
    MAX_CHUNK_TOKENS = 512
    SLIDE_STEP = 200
    MILVUS_DIM = 1536
    TOP_K = 3
    CANDIDATE_TOP_K = 20  # é‡æ’å‰çš„å€™é€‰æ•°é‡
    TEST_SAMPLE_NUM = None  # æ”¹ä¸ºNoneè¿è¡Œå…¨é‡æµ‹è¯•æ•°æ®

    # åˆå§‹åŒ–RAGç³»ç»Ÿ
    chunk_json_path = os.path.join(os.path.dirname(__file__), 'all_pdf_page_chunks.json')
    rag = SimpleRAG(
        chunk_json_path=chunk_json_path,
        max_chunk_tokens=MAX_CHUNK_TOKENS,
        slide_step=SLIDE_STEP,
        milvus_dim=MILVUS_DIM
    )
    rag.setup()

    # æµ‹è¯•1ï¼šæ­£å¸¸ç‰ˆæœ¬åŒ¹é…åœºæ™¯
    print("\n" + "=" * 60)
    print("ã€æµ‹è¯•1ï¼šæ­£å¸¸ç‰ˆæœ¬åŒ¹é…åœºæ™¯ã€‘")
    test_question1 = "2023å¹´ç¬¬ä¸‰å­£åº¦çš„å‡€åˆ©æ¶¦æ˜¯å¤šå°‘ï¼Ÿ"
    result1 = rag.generate_answer(test_question1, top_k=TOP_K, user_id="test_user_001")
    print(f"é—®é¢˜ï¼š{test_question1}")
    print(f"å›ç­”ï¼š{result1['answer'][:200]}...")
    print(
        f"ç‰ˆæœ¬ä¿¡æ¯ï¼šå½“å‰{result1['version_info']['embedding_version']}ï¼ŒåŒ¹é…ï¼š{result1['version_info']['version_matched']}")
    print(f"æ£€ç´¢ç±»å‹ï¼š{result1['note'].split('ï¼š')[1].split('ï¼Œ')[0]}")
    print(f"å®¡è®¡æ—¥å¿—IDï¼š{result1['audit_log_id']}")

    # æµ‹è¯•2ï¼šç‰ˆæœ¬ä¸åŒ¹é…ï¼ˆå›æ»šï¼‰åœºæ™¯
    print("\n" + "=" * 60)
    print("ã€æµ‹è¯•2ï¼šç‰ˆæœ¬ä¸åŒ¹é…ï¼ˆå›æ»šï¼‰åœºæ™¯ã€‘")
    # åˆ‡æ¢åˆ°ä¸€ä¸ªä¸å­˜åœ¨çš„ç‰ˆæœ¬ï¼ˆæ¨¡æ‹Ÿå›æ»šåç‰ˆæœ¬ä¸åŒ¹é…ï¼‰
    target_version = "bge-m3-v202404"  # å‡è®¾è¯¥ç‰ˆæœ¬åœ¨Milvusä¸­ä¸å­˜åœ¨
    switch_ok = rag.switch_embedding_version(target_version)
    if not switch_ok:
        # è‹¥åˆ‡æ¢å¤±è´¥ï¼Œä½¿ç”¨ä¸€ä¸ªå­˜åœ¨ä½†ä¸å½“å‰æ•°æ®ä¸åŒ¹é…çš„ç‰ˆæœ¬
        target_version = "unknown"
        rag.switch_embedding_version(target_version)

    test_question2 = "è”é‚¦åˆ¶è¯2024å¹´è¥ä¸šæ”¶å…¥åŒæ¯”å¢é•¿ç‡ä¸ºå¤šå°‘ï¼Ÿ"
    result2 = rag.generate_answer(test_question2, top_k=TOP_K, user_id="test_user_002")
    print(f"é—®é¢˜ï¼š{test_question2}")
    print(f"å›ç­”ï¼š{result2['answer'][:200]}...")
    print(
        f"ç‰ˆæœ¬ä¿¡æ¯ï¼šå½“å‰{result2['version_info']['embedding_version']}ï¼ŒåŒ¹é…ï¼š{result2['version_info']['version_matched']}")
    print(f"æ£€ç´¢ç±»å‹ï¼š{result2['note'].split('ï¼š')[1].split('ï¼Œ')[0]}")
    print(f"ç³»ç»Ÿæç¤ºï¼š{result2['note']}")
    print(f"å®¡è®¡æ—¥å¿—IDï¼š{result2['audit_log_id']}")

    # æµ‹è¯•3ï¼šæ¢å¤æ­£å¸¸ç‰ˆæœ¬
    print("\n" + "=" * 60)
    print("ã€æµ‹è¯•3ï¼šæ¢å¤æ­£å¸¸ç‰ˆæœ¬ã€‘")
    original_version = rag.embedding_model.get_available_versions()[0]
    rag.switch_embedding_version(original_version)
    test_question3 = "é‡‘èè¡Œä¸š2024å¹´èµ„äº§å‡å€¼æŸå¤±çš„ä¸»è¦æ„æˆæ˜¯ä»€ä¹ˆï¼Ÿ"
    result3 = rag.generate_answer(test_question3, top_k=TOP_K, user_id="test_user_003")
    print(f"é—®é¢˜ï¼š{test_question3}")
    print(f"å›ç­”ï¼š{result3['answer'][:200]}...")
    print(
        f"ç‰ˆæœ¬ä¿¡æ¯ï¼šå½“å‰{result3['version_info']['embedding_version']}ï¼ŒåŒ¹é…ï¼š{result3['version_info']['version_matched']}")
    print(f"æ£€ç´¢ç±»å‹ï¼š{result3['note'].split('ï¼š')[1].split('ï¼Œ')[0]}")
    print(f"å®¡è®¡æ—¥å¿—IDï¼š{result3['audit_log_id']}")

    # æ‰¹é‡æµ‹è¯•ï¼ˆå¯ç”¨å…¨é‡æ•°æ®ï¼‰
    FILL_UNANSWERED = True
    test_path = os.path.join(os.path.dirname(__file__), 'datas/å¤šæ¨¡æ€RAGå›¾æ–‡é—®ç­”æŒ‘æˆ˜èµ›æµ‹è¯•é›†.json')
    if os.path.exists(test_path):
        print("\n" + "=" * 60)
        with open(test_path, 'r', encoding='utf-8') as f:
            test_data = json.load(f)
        
        # å…¨é‡æµ‹è¯•é€»è¾‘
        if TEST_SAMPLE_NUM is None:
            print(f"å¼€å§‹å…¨é‡æµ‹è¯•ï¼ˆå…±{len(test_data)}æ¡æ ·æœ¬ï¼‰...")
            selected_indices = list(range(len(test_data)))
        else:
            print(f"å¼€å§‹æ‰¹é‡æµ‹è¯•ï¼ˆ{TEST_SAMPLE_NUM}æ¡æ ·æœ¬ï¼‰...")
            all_indices = list(range(len(test_data)))
            selected_indices = sorted(random.sample(all_indices, TEST_SAMPLE_NUM)) if len(
                test_data) > TEST_SAMPLE_NUM else all_indices

        results = []
        version_matched_count = 0
        retrieval_types = defaultdict(int)
        for idx in selected_indices:
            data_item = test_data[idx]
            question = data_item['question']
            print(f"\n[{selected_indices.index(idx) + 1}/{len(selected_indices)}] å¤„ç†: {question[:30]}...")
            result = rag.generate_answer(question, top_k=TOP_K, user_id=f"batch_user_{idx}")
            results.append((idx, result))
            # ç»Ÿè®¡ç‰ˆæœ¬åŒ¹é…å’Œæ£€ç´¢ç±»å‹
            if result['version_info']['version_matched']:
                version_matched_count += 1
            retrieval_type = result['note'].split('ï¼š')[1].split('ï¼Œ')[0]
            retrieval_types[retrieval_type] += 1

        # ç»Ÿè®¡ç»“æœ
        print(f"\næ‰¹é‡æµ‹è¯•ç‰ˆæœ¬å…¼å®¹æ€§ç»Ÿè®¡ï¼š")
        print(f"  - æ€»æ ·æœ¬æ•°ï¼š{len(results)}")
        print(f"  - ç‰ˆæœ¬åŒ¹é…æ•°ï¼š{version_matched_count}ï¼ˆ{round(version_matched_count / len(results) * 100, 1)}%ï¼‰")
        print(f"  - æ£€ç´¢ç±»å‹åˆ†å¸ƒï¼š{dict(retrieval_types)}")
        print(f"  - å½“å‰åµŒå…¥ç‰ˆæœ¬ï¼š{rag.current_embedding_version}")
        print(f"  - å¯ç”¨å‘é‡ç‰ˆæœ¬ï¼š{sorted(rag.vector_store.available_versions)}")
