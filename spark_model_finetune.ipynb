import torch
import re
import optuna
import numpy as np
import json
import os
from unsloth import FastLanguageModel
from trl import DPOTrainer, DPOConfig, SFTConfig, SFTTrainer
from peft import LoraConfig, get_peft_model
from datasets import Dataset
from optuna.samplers import TPESampler
from transformers import AutoModelForCausalLM, AutoTokenizer


# ===================== 依赖安装 =====================
def install_dependencies():
    import subprocess
    import sys
    subprocess.check_call([
        sys.executable, "-m", "pip", "install",
        "trl==0.7.4", "accelerate==0.24.1", "peft==0.6.2",
        "optuna", "unsloth", "datasets", "transformers>=4.36.0"
    ])
    print("✅ 依赖安装完成")

install_dependencies()


# ===================== 超参数配置 =====================
def get_optimal_hparams(model_size: str, task_type: str = "finance") -> dict:
    """根据模型规模和任务类型返回推荐超参数基线"""
    hparams = {
        "lora_rank": 16,
        "learning_rate": 5e-7,  # DPO学习率通常比SFT低一个数量级
        "weight_decay": 0.01,
        "num_train_epochs": 3,
        "batch_size": 4,
        "mini_batch_size": 2,
        "gradient_accumulation_steps": 4,
        "lr_scheduler_type": "cosine",
        "warmup_ratio": 0.05,
        "max_grad_norm": 0.5,
        "max_seq_length": 1024,
        "beta": 0.1  # DPO特有：控制KL散度权重
    }

    # 按模型规模调整
    if model_size == "7B":
        hparams.update({
            "lora_rank": 8,
            "batch_size": 4,
            "mini_batch_size": 2
        })
    elif model_size == "14B":
        hparams.update({
            "lora_rank": 32,
            "learning_rate": 3e-7,
            "batch_size": 2,
            "mini_batch_size": 1
        })

    # 金融领域特化
    if task_type == "finance":
        hparams.update({
            "weight_decay": 0.02,
            "warmup_ratio": 0.1,
            "max_grad_norm": 1.0,
            "num_train_epochs": 5,
            "max_seq_length": 2048,
            "beta": 0.05  # 金融场景降低KL权重，保护专业知识
        })
    return hparams


# ===================== 数据处理 =====================
def load_dpo_data(file_path: str, tokenizer) -> Dataset:
    """加载并格式化DPO偏好对数据（包含溯源信息）"""
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"DPO数据文件不存在: {file_path}")

    with open(file_path, "r", encoding="utf-8") as f:
        raw_data = json.load(f)

    # 数据格式校验：必须包含prompt、chosen、rejected字段
    required_fields = ["prompt", "chosen", "rejected"]
    for idx, item in enumerate(raw_data):
        for field in required_fields:
            if field not in item:
                raise ValueError(f"数据第{idx}条缺少必要字段: {field}")

    # 金融场景提示词增强（强调来源要求）
    finance_system_prompt = """你是专业金融分析师助手，回答必须基于提供的财报文档，内容需准确引用数据，同时明确标注信息来源（格式：来源: 文件名+页码）。错误的来源会导致严重后果。"""

    formatted_data = []
    for item in raw_data:
        # 构建带系统提示的完整prompt
        full_prompt = f"{finance_system_prompt}\n\n### 问题: {item['prompt']}\n### 回答:"

        # 确保chosen包含正确来源，rejected包含错误来源或内容
        formatted_data.append({
            "prompt": full_prompt,
            "chosen": item["chosen"],  # 格式示例："营业收入100亿。来源: 财报2024.pdf第5页"
            "rejected": item["rejected"]  # 格式示例："营业收入200亿。来源: 错误文档.pdf"
        })

    return Dataset.from_list(formatted_data)


# ===================== LoRA配置 =====================
def setup_lora(model, lora_rank: int, model_size: str):
    """配置LoRA适配器（针对Qwen2.5系列优化）"""
    # 根据模型规模选择目标模块
    if model_size in ["7B", "14B"]:
        target_modules = ["q_proj", "v_proj", "k_proj", "o_proj"]
    else:
        target_modules = ["q_proj", "v_proj"]

    lora_config = LoraConfig(
        r=lora_rank,
        lora_alpha=lora_rank * 2,  # 金融场景建议alpha=2*rank
        target_modules=target_modules,
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
        use_gradient_checkpointing=True
    )

    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()  # 输出可训练参数比例（通常<1%）
    return model


# ===================== 自定义DPO训练器（双目标对齐） =====================
class FinanceDPOTrainer(DPOTrainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        # 基础DPO损失计算
        loss, outputs = super().compute_loss(model, inputs, return_outputs=True)

        # 溯源准确性惩罚（金融场景核心优化）
        device = loss.device
        true_sources = inputs.get("true_source", None)
        if true_sources is None:
            return (loss, outputs) if return_outputs else loss

        # 提取chosen回答中的来源信息
        chosen_ids = inputs["chosen_ids"]
        chosen_texts = self.tokenizer.batch_decode(chosen_ids, skip_special_tokens=True)

        total_source_loss = torch.tensor(0.0, device=device)
        for chosen_text, true_src in zip(chosen_texts, true_sources):
            # 正则匹配来源格式（来源: XXX）
            src_match = re.search(r"来源: (.*)", chosen_text)
            pred_src = src_match.group(1).strip() if src_match else ""

            # 来源不匹配则增加惩罚（权重0.3，平衡内容与来源）
            if pred_src != true_src:
                total_source_loss += torch.tensor(0.3, device=device)

        source_loss = total_source_loss / len(true_sources)
        total_loss = loss + source_loss  # 融合DPO损失与来源惩罚
        return (total_loss, outputs) if return_outputs else total_loss


# ===================== 超参数搜索 =====================
def objective(trial: optuna.Trial, model_size: str, train_dataset: Dataset, tokenizer) -> float:
    """Optuna超参数搜索目标函数（适配DPO）"""
    # 定义搜索空间
    hparams = {
        "lora_rank": trial.suggest_int("lora_rank", 8, 64, step=8),
        "learning_rate": trial.suggest_float("learning_rate", 1e-7, 1e-6, log=True),
        "beta": trial.suggest_float("beta", 0.05, 0.2, step=0.05),
        "batch_size": trial.suggest_categorical("batch_size", [2, 4, 8]),
        "gradient_accumulation_steps": trial.suggest_categorical("gradient_accumulation_steps", [2, 4])
    }

    # 模型规模限制
    if model_size == "14B":
        hparams["batch_size"] = trial.suggest_categorical("batch_size", [2])

    # 加载模型（4bit量化节省显存）
    try:
        model = AutoModelForCausalLM.from_pretrained(
            "./model_train_outputs/sft_model",  # 基于SFT模型继续训练
            device_map="auto",
            torch_dtype=torch.bfloat16,
            load_in_4bit=True,
            trust_remote_code=True
        )
    except Exception as e:
        print(f"模型加载失败: {e}")
        return float("inf")

    # 配置LoRA
    model = setup_lora(model, hparams["lora_rank"], model_size)

    # 配置DPO参数
    dpo_config = DPOConfig(
        batch_size=hparams["batch_size"],
        mini_batch_size=hparams["batch_size"] // 2,
        learning_rate=hparams["learning_rate"],
        num_train_epochs=3,
        gradient_accumulation_steps=hparams["gradient_accumulation_steps"],
        warmup_ratio=0.1,
        weight_decay=0.02,
        lr_scheduler_type="cosine",
        output_dir=f"temp_dpo_{trial.number}",
        logging_steps=10,
        save_steps=50,
        remove_unused_columns=False,
        beta=hparams["beta"],
        fp16=True
    )

    # 初始化训练器
    trainer = FinanceDPOTrainer(
        model=model,
        ref_model=None,  # 显存不足时使用当前模型作为参考
        args=dpo_config,
        train_dataset=train_dataset,
        tokenizer=tokenizer,
        max_prompt_length=1536,  # 适配金融长文本
        max_response_length=512
    )

    # 训练
    trainer.train()

    # 简单评估（来源准确性）
    eval_score = evaluate_source_accuracy(model, train_dataset.select(range(100)), tokenizer)
    return eval_score


# ===================== 评估函数 =====================
def evaluate_source_accuracy(model, dataset, tokenizer) -> float:
    """评估回答内容准确性和来源可靠性"""
    model.eval()
    content_matches = []
    source_matches = []

    with torch.no_grad():
        for item in dataset:
            # 构建输入
            prompt = item["prompt"]
            input_ids = tokenizer(prompt, return_tensors="pt").to(model.device)

            # 生成回答
            outputs = model.generate(
                **input_ids,
                max_new_tokens=256,
                temperature=0.0,  # 确定性生成
                pad_token_id=tokenizer.pad_token_id
            )
            pred = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, "")

            # 内容匹配（金融术语重叠率）
            chosen_terms = set(item["chosen"].split())
            pred_terms = set(pred.split())
            content_overlap = len(chosen_terms & pred_terms) / len(chosen_terms) if chosen_terms else 0
            content_matches.append(content_overlap)

            # 来源匹配
            true_src = re.search(r"来源: (.*)", item["chosen"]).group(1).strip()
            pred_src = re.search(r"来源: (.*)", pred).group(1).strip() if re.search(r"来源: (.*)", pred) else ""
            source_matches.append(1 if pred_src == true_src else 0)

    # 综合评分（内容60% + 来源40%）
    return 0.6 * np.mean(content_matches) + 0.4 * np.mean(source_matches)


# ===================== 显存计算工具 =====================
def calculate_vram_usage(batch_size: int, seq_len: int, model_size: str) -> float:
    """估算显存占用（考虑DPO和4bit量化）"""
    model_base = {"7B": 14, "14B": 28}[model_size]  # FP16基础大小(GB)
    kv_cache = batch_size * seq_len * 4 * 2 / (1024**3)  # 注意力缓存
    gradient_saving = 0.6  # 梯度检查点节省40%
    quant_saving = 0.25    # 4bit量化节省75%
    return round((model_base * quant_saving + kv_cache) * gradient_saving * 1.5, 2)  # DPO额外多50%显存


# ===================== 主流程 =====================
def main():
    # 1. 配置参数
    model_size = "7B"  # 可选7B/14B
    task_type = "finance"
    dpo_data_path = "dpo_preference_data.json"  # DPO偏好对数据
    base_hparams = get_optimal_hparams(model_size, task_type)

    # 2. 加载Tokenizer
    print("加载分词器...")
    tokenizer = AutoTokenizer.from_pretrained(
        "Qwen/Qwen2.5-7B-Instruct",
        padding_side="right",
        trust_remote_code=True
    )
    tokenizer.pad_token = tokenizer.eos_token

    # 3. 加载DPO数据
    print("加载DPO数据...")
    try:
        train_dataset = load_dpo_data(dpo_data_path, tokenizer)
        print(f"加载DPO训练样本数: {len(train_dataset)}")
    except Exception as e:
        print(f"数据加载失败: {e}")
        return

    # 4. 超参数搜索
    print("开始超参数搜索...")
    study = optuna.create_study(
        direction="maximize",
        sampler=TPESampler(seed=3407),
        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)
    )
    study.optimize(
        lambda trial: objective(trial, model_size, train_dataset, tokenizer),
        n_trials=8,  # 搜索轮次
        show_progress_bar=True
    )

    print(f"最佳超参数: {study.best_params}")
    print(f"最佳评估分数: {study.best_value:.4f}")

    # 5. 最终训练
    best_hparams = {** base_hparams, **study.best_params}
    print("开始最终DPO训练...")

    # 显存预检查
    vram_usage = calculate_vram_usage(
        best_hparams["batch_size"],
        best_hparams["max_seq_length"],
        model_size
    )
    print(f"预估显存占用: {vram_usage} GB")

    # 加载基础模型
    model = AutoModelForCausalLM.from_pretrained(
        "./model_train_outputs/sft_model",
        device_map="auto",
        torch_dtype=torch.bfloat16,
        load_in_4bit=True,
        trust_remote_code=True
    )

    # 配置LoRA
    model = setup_lora(model, best_hparams["lora_rank"], model_size)

    # 配置DPO训练参数
    final_config = DPOConfig(
        batch_size=best_hparams["batch_size"],
        mini_batch_size=best_hparams["mini_batch_size"],
        learning_rate=best_hparams["learning_rate"],
        num_train_epochs=best_hparams["num_train_epochs"],
        gradient_accumulation_steps=best_hparams["gradient_accumulation_steps"],
        warmup_ratio=best_hparams["warmup_ratio"],
        weight_decay=best_hparams["weight_decay"],
        lr_scheduler_type=best_hparams["lr_scheduler_type"],
        max_grad_norm=best_hparams["max_grad_norm"],
        output_dir="./model_train_outputs/dpo_model",
        logging_steps=10,
        save_steps=100,
        remove_unused_columns=False,
        beta=best_hparams["beta"],
        fp16=True
    )

    # 初始化最终训练器
    final_trainer = FinanceDPOTrainer(
        model=model,
        ref_model=None,
        args=final_config,
        train_dataset=train_dataset,
        tokenizer=tokenizer,
        max_prompt_length=best_hparams["max_seq_length"],
        max_response_length=512
    )

    # 执行训练
    print("启动训练...")
    trainer_stats = final_trainer.train()
    print(f"训练完成，耗时: {trainer_stats.metrics['train_runtime']/60:.2f} 分钟")

    # 6. 最终评估
    final_score = evaluate_source_accuracy(model, train_dataset.select(range(200)), tokenizer)
    print(f"最终模型综合评分: {final_score:.4f}")

    # 7. 模型保存
    final_trainer.save_model(final_config.output_dir)
    tokenizer.save_pretrained(final_config.output_dir)
    print(f"模型保存至: {final_config.output_dir}")

    # 8. 测试示例
    test_prompt = """你是专业金融分析师助手，回答必须基于提供的财报文档，内容需准确引用数据，同时明确标注信息来源（格式：来源: 文件名+页码）。错误的来源会导致严重后果。

### 问题: 联邦制药2024年营业收入和净利润分别是多少？
### 回答:"""

    input_ids = tokenizer(test_prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(** input_ids, max_new_tokens=256, temperature=0.0)
    print("测试输出:", tokenizer.decode(outputs[0], skip_special_tokens=True))


if __name__ == "__main__":
    main()
