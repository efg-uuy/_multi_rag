import json
import os
import re
import random
import logging
import asyncio
import pandas as pd
import nlpaug.augmenter.word as naw
from typing import List, Dict, Any, Optional, Tuple
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
import torch

# 导入项目内图像分析工具（假设已存在）
from async_image_analysis import AsyncImageAnalysis, ChartTypeClassifier

# 配置日志
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler("finance_data_process.log"), logging.StreamHandler()]
)

# 金融领域实体词表（增强领域对齐）
FINANCIAL_ENTITIES = {
    "指标": [
        "营收", "营业收入", "净利润", "毛利润", "毛利率", "净利率",
        "资产负债率", "ROE", "ROA", "同比增长率", "环比增长率",
        "每股收益", "市盈率", "市净率", "现金流", "资产总额",
        "负债总额", "股东权益", "研发费用", "销售费用"
    ],
    "主体": [
        "公司", "企业", "银行", "基金", "股票", "债券",
        "上市公司", "母公司", "子公司", "金融机构", "券商",
        "保险公司", "信托公司"
    ],
    "文档类型": [
        "财报", "研报", "年报", "季报", "中报", "公告",
        "招股书", "债券募集说明书", "评级报告", "尽职调查报告"
    ],
    "时间维度": ["年度", "季度", "月度", "半年度", "同比", "环比"],
    "图表类型": ["柱状图", "折线图", "饼图", "表格", "散点图", "热力图"]
}

# 数据增强器缓存
class AugmenterCache:
    _cn_synonym_aug = None
    _cn_context_aug = None

    @classmethod
    def get_cn_synonym_aug(cls):
        """中文同义词替换增强器"""
        if cls._cn_synonym_aug is None:
            try:
                cls._cn_synonym_aug = naw.ContextualWordEmbsAug(
                    model_path='hfl/chinese-roberta-wwm-ext',
                    action="substitute",
                    aug_p=0.3,
                    device='cuda' if torch.cuda.is_available() else 'cpu'
                )
                logging.info("成功加载中文同义词增强器")
            except Exception as e:
                logging.warning(f"中文同义词增强器加载失败，使用基础增强: {str(e)}")
                cls._cn_synonym_aug = naw.SynonymAug(
                    aug_src='wordnet',
                    aug_p=0.2
                )
        return cls._cn_synonym_aug

    @classmethod
    def get_cn_context_aug(cls):
        """中文上下文扰动增强器"""
        if cls._cn_context_aug is None:
            try:
                cls._cn_context_aug = naw.RandomWordAug(
                    action='swap',
                    aug_p=0.1
                )
                logging.info("成功加载中文上下文增强器")
            except Exception as e:
                logging.warning(f"中文上下文增强器加载失败: {str(e)}")
                cls._cn_context_aug = None
        return cls._cn_context_aug


# 模型缓存（复用避免重复加载）
class ModelCache:
    _qg_model = None
    _paraphrase_model = None
    _tokenizer = None
    _image_analyzer = None
    _chart_classifier = None

    @classmethod
    def get_qg_pipeline(cls):
        if cls._qg_model is None:
            try:
                cls._qg_model = pipeline(
                    "text2text-generation",
                    model="lmqg/t5-base-finance-qg",
                    device=0 if torch.cuda.is_available() else -1
                )
                logging.info("成功加载金融问题生成模型")
            except Exception as e:
                logging.error(f"金融问题生成模型加载失败: {str(e)}")
                raise
        return cls._qg_model

    @classmethod
    def get_paraphrase_model(cls):
        if cls._paraphrase_model is None:
            try:
                cls._tokenizer = AutoTokenizer.from_pretrained("Vamsi/T5_Paraphrase_Paws")
                cls._paraphrase_model = AutoModelForSeq2SeqLM.from_pretrained(
                    "Vamsi/T5_Paraphrase_Paws"
                ).to("cuda" if torch.cuda.is_available() else "cpu")
                logging.info("成功加载同义改写模型")
            except Exception as e:
                logging.error(f"同义改写模型加载失败: {str(e)}")
                raise
        return cls._paraphrase_model, cls._tokenizer

    @classmethod
    def get_image_analyzer(cls):
        if cls._image_analyzer is None:
            try:
                cls._image_analyzer = AsyncImageAnalysis(provider="zhipu")
                logging.info("成功初始化图像分析器")
            except Exception as e:
                logging.warning(f"图像分析器初始化失败，多模态功能受限: {str(e)}")
                cls._image_analyzer = None
        return cls._image_analyzer

    @classmethod
    def get_chart_classifier(cls, weight_path: Optional[str] = None):
        if cls._chart_classifier is None:
            try:
                cls._chart_classifier = ChartTypeClassifier(model_weight_path=weight_path)
                logging.info("成功初始化图表分类器")
            except Exception as e:
                logging.warning(f"图表分类器初始化失败，图表类型校验功能禁用: {str(e)}")
                cls._chart_classifier = None
        return cls._chart_classifier


def extract_finance_entities(text: str) -> Dict[str, List[str]]:
    """增强版金融实体提取"""
    entities = {k: [] for k in FINANCIAL_ENTITIES.keys()}
    text_lower = text.lower()

    for category, keywords in FINANCIAL_ENTITIES.items():
        for keyword in keywords:
            pattern = re.compile(rf"\b{re.escape(keyword)}\b|{re.escape(keyword)}[率额值度表]", re.IGNORECASE)
            if pattern.search(text_lower):
                entities[category].append(keyword)

    for k in entities:
        entities[k] = list(dict.fromkeys(entities[k]))
    return entities


def is_answer_consistent(question: str, answer: str, context: str = "") -> bool:
    """金融领域答案一致性校验"""
    if not answer.strip():
        logging.warning("过滤空答案样本")
        return False

    # 检查来源引用格式
    source_pattern = r"([\u4e00-\u9fa5\w]+\.pdf).*?(第\d+页|Page \d+)"
    if not re.search(source_pattern, answer):
        question_ents = extract_finance_entities(question)
        answer_ents = extract_finance_entities(answer)
        context_ents = extract_finance_entities(context) if context else {}

        shared_indicators = set(question_ents["指标"]) & set(answer_ents["指标"])
        if context and not shared_indicators:
            logging.warning(f"答案与问题无共享指标: {question[:30]}...")
            return False

    # 检查数值合理性
    num_pattern = r"(\d+,\d+|\d+)(\.\d+)?(万|千|百|亿|万亿)?(元|美元|%)"
    if re.search(num_pattern, answer):
        for indicator in FINANCIAL_ENTITIES["指标"]:
            if indicator in question and "率" in indicator:
                if "%" not in answer:
                    logging.warning(f"比率指标答案缺少百分号: {indicator}")
                    return False

    return True


def is_hard_case(item: Dict[str, Any]) -> Tuple[bool, float]:
    """难例判断与权重设置"""
    question = item.get("question", "").lower()
    context = item.get("context", "").lower()
    weight = 1.0

    # 图表类问题（高优先级硬例）
    chart_keywords = FINANCIAL_ENTITIES["图表类型"] + ["图表", "图像", "图片", "图示"]
    if any(kw in question for kw in chart_keywords):
        return True, 2.5

    # 复杂计算类问题
    calc_keywords = ["同比", "环比", "增长率", "占比", "合计", "平均", "总和"]
    if any(kw in question for kw in calc_keywords):
        return True, 2.0

    # 多指标交叉分析
    question_ents = extract_finance_entities(question)
    if len(question_ents["指标"]) >= 3:
        return True, 1.8

    return False, weight


def paraphrase_question(question: str, num_variations: int = 2) -> List[str]:
    """生成同义问题（数据增强）"""
    model, tokenizer = ModelCache.get_paraphrase_model()
    inputs = tokenizer(
        f"paraphrase: {question}",
        padding="max_length",
        max_length=128,
        truncation=True,
        return_tensors="pt"
    ).to(model.device)

    outputs = model.generate(
        **inputs,
        max_length=128,
        num_return_sequences=num_variations,
        do_sample=True,
        top_k=50,
        temperature=0.7
    )

    variations = [
        tokenizer.decode(output, skip_special_tokens=True).strip()
        for output in outputs
    ]
    return list(dict.fromkeys(variations))


def augment_with_nlpaug(question: str, num_variations: int = 2) -> List[str]:
    """基于nlpaug的中文问题增强"""
    augments = []
    synonym_aug = AugmenterCache.get_cn_synonym_aug()
    context_aug = AugmenterCache.get_cn_context_aug()

    # 同义词替换增强
    for _ in range(num_variations):
        try:
            aug_question = synonym_aug.augment(question)
            if aug_question != question:
                augments.append(aug_question)
        except Exception as e:
            logging.warning(f"同义词增强失败: {str(e)}")

    # 上下文扰动增强
    if context_aug:
        for _ in range(num_variations):
            try:
                aug_question = context_aug.augment(question)
                if aug_question != question and aug_question not in augments:
                    augments.append(aug_question)
            except Exception as e:
                logging.warning(f"上下文增强失败: {str(e)}")

    return list(dict.fromkeys(augments))[:num_variations*2]


def generate_hard_negatives(question: str, correct_answer: str, context: str) -> List[str]:
    """生成硬负例（相似问题但错误的答案）"""
    negatives = []
    if not context:
        return negatives

    context_chunks = re.split(r"。|；|！|\n", context)
    context_chunks = [c.strip() for c in context_chunks if len(c.strip()) > 10]

    correct_len = len(correct_answer)
    for chunk in context_chunks:
        if abs(len(chunk) - correct_len) < 50 and chunk not in correct_answer:
            ents = extract_finance_entities(correct_answer)
            for indicator in ents["指标"][:1]:
                chunk = re.sub(indicator, f"错误{indicator}", chunk)
            negatives.append(chunk)
            if len(negatives) >= 2:
                break
    return negatives


def augment_data(context: str, original_question: str, original_answer: str) -> List[Dict[str, Any]]:
    """综合数据增强"""
    augmented = []
    qg_pipeline = ModelCache.get_qg_pipeline()

    # 1. nlpaug增强
    try:
        nlpaug_variations = augment_with_nlpaug(original_question)
        for var in nlpaug_variations:
            augmented.append({
                "type": "nlpaug_variation",
                "context": context,
                "question": var,
                "answer": original_answer,
                "is_positive": True
            })
    except Exception as e:
        logging.error(f"nlpaug增强失败: {str(e)}")

    # 2. 同义问题改写
    try:
        paraphrased = paraphrase_question(original_question)
        for p in paraphrased:
            augmented.append({
                "type": "paraphrase",
                "context": context,
                "question": p,
                "answer": original_answer,
                "is_positive": True
            })
    except Exception as e:
        logging.error(f"同义改写失败: {str(e)}")

    # 3. 生成新问题
    try:
        generated_questions = qg_pipeline(
            context,
            max_length=100,
            num_return_sequences=2,
            temperature=0.8
        )
        for q in generated_questions:
            augmented.append({
                "type": "new_question",
                "context": context,
                "question": q["generated_text"],
                "answer": original_answer,
                "is_positive": True
            })
    except Exception as e:
        logging.error(f"新问题生成失败: {str(e)}")

    # 4. 生成硬负例
    try:
        hard_negatives = generate_hard_negatives(original_question, original_answer, context)
        for neg in hard_negatives:
            augmented.append({
                "type": "hard_negative",
                "context": context,
                "question": original_question,
                "answer": neg,
                "is_positive": False
            })
    except Exception as e:
        logging.error(f"硬负例生成失败: {str(e)}")

    return augmented


async def build_multimodal_context(
    base_context: str,
    image_path: Optional[str],
    question: str
) -> Tuple[str, Dict[str, Any]]:
    """构建包含图像描述的多模态上下文"""
    multimodal_meta = {"has_image": False, "image_analysis": None, "chart_check": None}
    if not image_path or not os.path.exists(image_path):
        return base_context, multimodal_meta

    multimodal_meta["has_image"] = True
    image_analyzer = ModelCache.get_image_analyzer()
    chart_classifier = ModelCache.get_chart_classifier()

    try:
        analysis_result = await image_analyzer.analyze(image_path)
        image_desc = analysis_result.get("description", "无法解析图片内容")
        multimodal_meta["image_analysis"] = {
            "description": image_desc,
            "confidence": analysis_result.get("confidence", 0.0)
        }

        if chart_classifier:
            # 假设存在图表类型一致性检查函数
            is_consistent, check_msg = chart_classifier.check_consistency(
                image_path=image_path,
                text_description=image_desc
            )
            multimodal_meta["chart_check"] = {
                "consistent": is_consistent,
                "message": check_msg
            }

        full_context = (
            f"【图像内容描述】：{image_desc}\n"
            f"【文本上下文】：{base_context}"
        )
        return full_context, multimodal_meta

    except Exception as e:
        logging.warning(f"图像分析失败，使用原始上下文: {str(e)}")
        return base_context, multimodal_meta


def generate_rejected_answer(chosen: str, correct_file: str, correct_page: str) -> str:
    """生成针对性劣质答案（贴合评分痛点）"""
    content_only = re.sub(r"（来源：.*?）", "", chosen).strip()
    reject_type = random.choice(["wrong_page", "missing_source", "missing_multimodal"])

    if reject_type == "wrong_page":
        # 页码错误（±1或±2）
        wrong_page = str(int(correct_page) + random.choice([-2, -1, 1, 2]))
        return f"{content_only}（来源：{correct_file} P{wrong_page}）"

    elif reject_type == "missing_source":
        # 缺失来源信息
        return content_only

    elif reject_type == "missing_multimodal":
        # 遗漏图表/表格数据
        if "表格" in chosen or "图" in chosen:
            return re.sub(r"（表格.*?）|（图.*?）", "", content_only).strip() + f"（来源：{correct_file} P{correct_page}）"
        else:
            # 内容缺失部分信息
            return content_only[:len(content_only)//2] + "..." + f"（来源：{correct_file} P{correct_page}）"


def load_alpaca_data(alpaca_path: str) -> List[Dict]:
    """加载Alpaca格式数据"""
    if not os.path.exists(alpaca_path):
        raise FileNotFoundError(f"Alpaca格式数据文件不存在: {alpaca_path}")
    with open(alpaca_path, "r", encoding="utf-8") as f:
        return json.load(f)


def build_dpo_data(alpaca_data: List[Dict], output_path: str, chunk_data_path: str = "all_pdf_page_chunks.json"):
    """构建DPO偏好对数据（核心功能）"""
    # 加载PDF分页上下文
    chunk_context = {}
    if os.path.exists(chunk_data_path):
        with open(chunk_data_path, "r", encoding="utf-8") as f:
            chunks = json.load(f)
            for chunk in chunks:
                key = f"{chunk['metadata']['file_name']}_page_{chunk['metadata']['page']}"
                chunk_context[key] = chunk['content']
        logging.info(f"已加载 {len(chunk_context)} 条PDF分页上下文")

    dpo_data = []
    for item in alpaca_data:
        prompt = item["instruction"]
        chosen = item["output"]
        source_match = re.search(r"来源：(.*?\.pdf) P(\d+)", chosen)

        if not source_match:
            logging.warning(f"跳过无溯源信息的样本：{prompt[:20]}...")
            continue

        correct_file = source_match.group(1)
        correct_page = source_match.group(2)
        context_key = f"{correct_file}_page_{correct_page}"
        base_context = chunk_context.get(context_key, "")

        # 过滤无效样本
        if not is_answer_consistent(prompt, chosen, base_context):
            logging.warning(f"跳过不一致样本: {prompt[:30]}...")
            continue

        # 生成劣质答案
        rejected = generate_rejected_answer(chosen, correct_file, correct_page)

        # 构建多模态上下文
        try:
            image_path = item.get("metadata", {}).get("image_path")
            full_context, multimodal_meta = asyncio.run(
                build_multimodal_context(
                    base_context=base_context,
                    image_path=image_path,
                    question=prompt
                )
            )
        except Exception as e:
            logging.warning(f"多模态处理失败: {str(e)}")
            full_context = base_context
            multimodal_meta = {"has_image": False}

        # 难例判断
        is_hard, weight = is_hard_case({"question": prompt, "context": full_context})

        # 加入DPO样本
        dpo_sample = {
            "prompt": prompt,
            "chosen": chosen,
            "rejected": rejected,
            "context": full_context,  # 保留上下文供模型参考
            "metadata": {
                "correct_file": correct_file,
                "correct_page": correct_page,
                "is_hard_case": is_hard,
                "weight": weight,
                **multimodal_meta
            }
        }
        dpo_data.append(dpo_sample)

    # 保存结果
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(dpo_data, f, ensure_ascii=False, indent=2)
    logging.info(f"生成DPO偏好对数据 {len(dpo_data)} 条，保存至 {output_path}")
    return dpo_data


def main():
    import argparse
    parser = argparse.ArgumentParser(description="金融领域DPO偏好数据生成工具")
    parser.add_argument("--alpaca_path", default="alpaca_train_data.json", help="Alpaca格式训练数据路径")
    parser.add_argument("--chunk_data", default="all_pdf_page_chunks.json", help="PDF分页上下文路径")
    parser.add_argument("--output_path", default="dpo_preference_data.json", help="DPO数据输出路径")
    parser.add_argument("--chart_weight", default=None, help="图表分类器权重路径")
    args = parser.parse_args()

    # 初始化模型缓存
    ModelCache.get_image_analyzer()
    ModelCache.get_chart_classifier(args.chart_weight)

    # 执行DPO数据生成
    alpaca_data = load_alpaca_data(args.alpaca_path)
    build_dpo_data(
        alpaca_data=alpaca_data,
        output_path=args.output_path,
        chunk_data_path=args.chunk_data
    )


if __name__ == "__main__":
    main()
