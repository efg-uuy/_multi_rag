import json
import os
import time
import uuid
import asyncio
import concurrent.futures
import random
import torch
import numpy as np
from rank_bm25 import BM25Okapi  # æ–°å¢ï¼šBM25å…³é”®è¯æ£€ç´¢
from typing import List, Dict, Any, Optional, Tuple
from tqdm import tqdm
import sys
import re
from collections import defaultdict

sys.path.append(os.path.dirname(__file__))
from get_text_embedding import get_text_embedding

# ç¡®ä¿ä¾èµ–å®‰è£…æç¤º
required_packages = {
    "python-dotenv": "pip install python-dotenv",
    "openai": "pip install openai",
    "redis": "pip install redis",
    "pymilvus": "pip install pymilvus==2.4.3",
    "sentence-transformers": "pip install sentence-transformers",
    "scikit-learn": "pip install scikit-learn",
    "FlagEmbedding": "pip install FlagEmbedding",
    "rank-bm25": "pip install rank-bm25"  # æ–°å¢ï¼šBM25ä¾èµ–
}
for pkg, install_cmd in required_packages.items():
    try:
        __import__(pkg.replace("-", "_"))
    except ImportError:
        print(f"è­¦å‘Šï¼šæœªå®‰è£…{pkg}ï¼Œè¯·æ‰§è¡Œå‘½ä»¤ï¼š{install_cmd}")

# å¯¼å…¥é‡æ’æ¨¡å‹
from FlagEmbedding import FlagReranker

# åŸºç¡€ä¾èµ–å¯¼å…¥
try:
    from dotenv import load_dotenv
except ImportError:
    load_dotenv = lambda: None

try:
    from openai import OpenAI
except ImportError:
    class OpenAI:
        def __init__(self, *args, **kwargs):
            raise ImportError("è¯·å®‰è£…openaiåŒ…ï¼špip install openai")

        def chat(self, *args, **kwargs):
            pass

try:
    import redis
except ImportError:
    class MockRedis:
        def __init__(self, *args, **kwargs):
            self.data = {}

        def setex(self, key, expire, value):
            self.data[key] = (value, time.time() + expire)

        def get(self, key):
            val, exp = self.data.get(key, (None, 0))
            return val if time.time() < exp else None


    redis = MockRedis

# Milvusç›¸å…³å¯¼å…¥
try:
    from pymilvus import (
        connections,
        FieldSchema, CollectionSchema, DataType,
        Collection,
        utility,
        Partition
    )

    MILVUS_AVAILABLE = True
except ImportError:
    MILVUS_AVAILABLE = False
    print("âš ï¸ Milvusæœªå®‰è£…ï¼Œå°†å›é€€åˆ°SimpleVectorStoreï¼ˆæ€§èƒ½è¾ƒå·®ï¼‰")

# å…³é”®è¯æ£€ç´¢ä¾èµ–
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity

    TFIDF_AVAILABLE = True
except ImportError:
    TFIDF_AVAILABLE = False
    print("âš ï¸ scikit-learnæœªå®‰è£…ï¼Œå…œåº•å…³é”®è¯æ£€ç´¢åŠŸèƒ½å—é™")

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


# ===================== 1. å®¡è®¡æ—¥å¿—å·¥å…·ç±» =====================
class AuditLogManager:
    def __init__(self, redis_host: str = "localhost", redis_port: int = 6379, redis_db: int = 0):
        try:
            self.redis_client = redis.Redis(
                host=redis_host, port=redis_port, db=redis_db, decode_responses=True
            )
            self.redis_client.ping()
            self.using_redis = True
            print("âœ… Rediså®¡è®¡æ—¥å¿—è¿æ¥æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ Redisè¿æ¥å¤±è´¥ï¼Œä½¿ç”¨å†…å­˜æ¨¡æ‹Ÿ: {str(e)}")
            self.redis_client = {}
            self.using_redis = False

        self.db_config = {
            "host": os.getenv("DB_HOST", "localhost"),
            "port": os.getenv("DB_PORT", 3306),
            "user": os.getenv("DB_USER", "root"),
            "password": os.getenv("DB_PASSWORD", ""),
            "database": os.getenv("AUDIT_DB_NAME", "audit_logs")
        }

    def create_pre_log(self, question: str, metadata: Optional[Dict[str, Any]] = None) -> str:
        log_id = uuid.uuid4().hex
        log_data = {
            "log_id": log_id,
            "question": question,
            "status": "processing",
            "timestamp": time.time(),
            "create_time": time.strftime("%Y-%m-%d %H:%M:%S"),
            "metadata": metadata or {}
        }
        if self.using_redis:
            self.redis_client.setex(f"audit_log:{log_id}", 3600, json.dumps(log_data, ensure_ascii=False))
        else:
            self.redis_client[f"audit_log:{log_id}"] = (log_data, time.time() + 3600)
        return log_id

    def update_log_with_answer(self, log_id: str, answer: str, retrieval_chunks: List[Dict[str, Any]],
                               status: str = "completed", model_version: str = "unknown",
                               retrieval_type: str = "vector") -> bool:
        if self.using_redis:
            log_str = self.redis_client.get(f"audit_log:{log_id}")
            if not log_str:
                print(f"âŒ æ—¥å¿—ID {log_id} ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ")
                return False
            log_data = json.loads(log_str)
        else:
            log_entry = self.redis_client.get(f"audit_log:{log_id}")
            if not log_entry or time.time() > log_entry[1]:
                print(f"âŒ æ—¥å¿—ID {log_id} ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ")
                return False
            log_data = log_entry[0]

        log_data.update({
            "answer": answer,
            "retrieval_chunks": [
                {"id": c.get("id"), "metadata": c.get("metadata"), "model_version": c.get("model_version", "unknown")}
                for c in retrieval_chunks],
            "status": status,
            "complete_time": time.strftime("%Y-%m-%d %H:%M:%S"),
            "process_duration": round(time.time() - log_data["timestamp"], 3),
            "embedding_model_version": model_version,
            "retrieval_type": retrieval_type
        })

        if self.using_redis:
            self.redis_client.setex(f"audit_log:{log_id}", 86400, json.dumps(log_data, ensure_ascii=False))
        else:
            self.redis_client[f"audit_log:{log_id}"] = (log_data, time.time() + 86400)
        return True

    async def write_audit_log_to_db(self, log_id: str, retries: int = 3) -> None:
        for attempt in range(retries):
            try:
                if self.using_redis:
                    log_str = self.redis_client.get(f"audit_log:{log_id}")
                    if not log_str:
                        print(f"âŒ å¼‚æ­¥è½ç›˜å¤±è´¥ï¼šæ—¥å¿—ID {log_id} ä¸å­˜åœ¨")
                        return
                    log_data = json.loads(log_str)
                else:
                    log_entry = self.redis_client.get(f"audit_log:{log_id}")
                    if not log_entry or time.time() > log_entry[1]:
                        print(f"âŒ å¼‚æ­¥è½ç›˜å¤±è´¥ï¼šæ—¥å¿—ID {log_id} ä¸å­˜åœ¨")
                        return
                    log_data = log_entry[0]

                print(f"ğŸ“ å¼‚æ­¥è½ç›˜æ—¥å¿— {log_id} åˆ°æ•°æ®åº“: {log_data['question'][:20]}...")
                await asyncio.sleep(0.1)
                if self.using_redis:
                    self.redis_client.delete(f"audit_log:{log_id}")
                break
            except Exception as e:
                if attempt < retries - 1:
                    print(f"âš ï¸ å¼‚æ­¥è½ç›˜å°è¯• {attempt + 1} å¤±è´¥ï¼Œé‡è¯•ä¸­: {str(e)}")
                    await asyncio.sleep(1)
                else:
                    print(f"âŒ å¼‚æ­¥è½ç›˜æœ€ç»ˆå¤±è´¥ï¼ˆ{retries}æ¬¡å°è¯•ï¼‰: {str(e)}")
                    if self.using_redis:
                        self.redis_client.lpush("audit_log_failed", log_id)


# ===================== 2. Chunkå¤„ç†å·¥å…· =====================
def smart_truncate(text: str, max_tokens: int = 512) -> str:
    tokens = text.split()
    if len(tokens) <= max_tokens:
        return text
    number_positions = [i for i, token in enumerate(tokens) if any(c.isdigit() for c in token)]
    center_idx = number_positions[len(number_positions) // 2] if number_positions else len(tokens) // 2
    half_window = max_tokens // 2
    start_idx = max(0, center_idx - half_window)
    end_idx = min(len(tokens), start_idx + max_tokens)
    return " ".join(tokens[start_idx:end_idx])


def slide_window_split(text: str, max_tokens: int = 512, slide_step: int = 200) -> List[str]:
    tokens = text.split()
    total_tokens = len(tokens)
    if total_tokens <= max_tokens:
        return [text]
    sub_chunks = []
    start_idx = 0
    while start_idx < total_tokens:
        end_idx = min(start_idx + max_tokens, total_tokens)
        sub_tokens = tokens[start_idx:end_idx]
        sub_chunk = " ".join(sub_tokens)
        sub_chunks.append(smart_truncate(sub_chunk, max_tokens))
        start_idx += slide_step
        if start_idx + max_tokens > total_tokens and start_idx < total_tokens:
            start_idx = max(0, total_tokens - max_tokens)
    return list(dict.fromkeys(sub_chunks))


def process_long_chunk(chunk: Dict[str, Any], max_tokens: int = 512, slide_step: int = 200) -> List[Dict[str, Any]]:
    raw_content = chunk["content"]
    metadata = chunk["metadata"].copy()
    sub_contents = slide_window_split(raw_content, max_tokens, slide_step)
    sub_chunks = []
    for idx, sub_content in enumerate(sub_contents):
        sub_metadata = metadata.copy()
        sub_metadata["sub_chunk_idx"] = idx
        sub_metadata["total_sub_chunks"] = len(sub_contents)
        sub_chunks.append({"content": sub_content, "metadata": sub_metadata})
    return sub_chunks


# ===================== 3. BM25æ£€ç´¢å®ç°ï¼ˆæ–°å¢æ ¸å¿ƒï¼‰ =====================
class BM25Retriever:
    def __init__(self, chunks):
        self.tokenized_corpus = [chunk["content"].split() for chunk in chunks]
        self.bm25 = BM25Okapi(self.tokenized_corpus)
        self.chunks = chunks

    def retrieve(self, query, top_k=5):
        tokenized_query = query.split()
        scores = self.bm25.get_scores(tokenized_query)
        top_indices = scores.argsort()[-top_k:][::-1]
        return [self.chunks[i] for i in top_indices]


# ===================== 4. æ··åˆæ£€ç´¢ä¸RRFèåˆï¼ˆæ–°å¢æ ¸å¿ƒï¼‰ =====================
def hybrid_retrieve(query, vector_retriever, bm25_retriever, vector_store, embedding_version, top_k=5,
                    candidate_top_k=10):
    # å‘é‡æ£€ç´¢ç»“æœ
    vector_results, version_matched = vector_store.search_with_version(
        query_embedding=vector_retriever,
        model_version=embedding_version,
        top_k=candidate_top_k
    )

    # BM25æ£€ç´¢ç»“æœ
    bm25_results = bm25_retriever.retrieve(query, top_k=candidate_top_k)

    # RRFèåˆï¼ˆå€’æ•°æ’ååˆ†æ•°ï¼‰
    rrf_scores = defaultdict(float)
    k = 60  # RRFè¶…å‚æ•°
    for rank, chunk in enumerate(vector_results, 1):
        rrf_scores[chunk["id"]] += 1 / (k + rank)
    for rank, chunk in enumerate(bm25_results, 1):
        rrf_scores[chunk["id"]] += 1 / (k + rank)

    # æŒ‰åˆ†æ•°æ’åºå–top_k
    sorted_ids = sorted(rrf_scores.keys(), key=lambda x: rrf_scores[x], reverse=True)[:top_k]

    # åˆå¹¶ç»“æœå¹¶å»é‡
    combined_chunks = vector_results + bm25_results
    unique_chunks = {chunk["id"]: chunk for chunk in combined_chunks}
    return [unique_chunks[id] for id in sorted_ids if id in unique_chunks], version_matched


# ===================== 5. å‘é‡åº“ä¼˜åŒ–ï¼šMilvusVectorStore =====================
class MilvusVectorStore:
    def __init__(self, collection_name: str = "rag_finance_chunks", dim: int = 1536):
        self.collection_name = collection_name
        self.dim = dim
        self.collection = None
        self._connect_milvus()
        self._create_collection_with_version()
        self._load_all_versions()

    def _connect_milvus(self) -> None:
        if not MILVUS_AVAILABLE:
            self.collection = None
            return
        milvus_host = os.getenv("MILVUS_HOST", "localhost")
        milvus_port = os.getenv("MILVUS_PORT", "19530")
        try:
            connections.connect(
                alias="default",
                host=milvus_host,
                port=milvus_port
            )
            print(f"âœ… æˆåŠŸè¿æ¥Milvusï¼š{milvus_host}:{milvus_port}")
        except Exception as e:
            print(f"âš ï¸ Milvusè¿æ¥å¤±è´¥ï¼Œå°†å›é€€åˆ°å†…å­˜æ¨¡å¼ï¼š{str(e)}")
            self.collection = None
            self.in_memory_chunks = []
            self.in_memory_embeddings = []
            self.in_memory_versions = []

    def _create_collection_with_version(self) -> None:
        if not MILVUS_AVAILABLE or self.collection:
            return
        if not utility.has_collection(self.collection_name, using="default"):
            fields = [
                FieldSchema(name="id", dtype=DataType.VARCHAR, is_primary=True, max_length=64),
                FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=self.dim),
                FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=4096),
                FieldSchema(name="metadata", dtype=DataType.JSON),
                FieldSchema(name="embedding_model_version", dtype=DataType.VARCHAR, max_length=64)
            ]
            schema = CollectionSchema(fields=fields, description="RAGé‡‘èåœºæ™¯Chunkå‘é‡é›†åˆï¼ˆå¸¦ç‰ˆæœ¬ç»‘å®šï¼‰")
            self.collection = Collection(name=self.collection_name, schema=schema, using="default")

            index_params = {
                "index_type": "IVF_HNSW",
                "metric_type": "IP",
                "params": {"nlist": 2048, "M": 8, "efConstruction": 64}
            }
            self.collection.create_index(field_name="embedding", index_params=index_params)
            print(f"âœ… Milvusé›†åˆ[{self.collection_name}]åŠç‰ˆæœ¬å­—æ®µåˆ›å»ºå®Œæˆ")
        else:
            self.collection = Collection(name=self.collection_name, using="default")
            schema = self.collection.schema
            if not any(f.name == "embedding_model_version" for f in schema.fields):
                print("âš ï¸ ç°æœ‰Milvusé›†åˆç¼ºå°‘ç‰ˆæœ¬å­—æ®µï¼Œæ­£åœ¨æ·»åŠ ...")
                self.collection.add_field(
                    FieldSchema(name="embedding_model_version", dtype=DataType.VARCHAR, max_length=64,
                                default_value="unknown")
                )
            print(f"âœ… Milvusé›†åˆ[{self.collection_name}]å·²å­˜åœ¨ï¼Œç›´æ¥åŠ è½½")

        self.collection.load()

    def _load_all_versions(self) -> None:
        self.available_versions = set()
        if not MILVUS_AVAILABLE or not self.collection:
            self.available_versions = set(self.in_memory_versions) if hasattr(self, "in_memory_versions") else set()
            return

        try:
            res = self.collection.query(
                expr="1==1",
                output_fields=["embedding_model_version"],
                limit=0
            )
            self.available_versions = {item["embedding_model_version"] for item in res if
                                       item["embedding_model_version"]}
            print(f"âœ… åŠ è½½Milvusä¸­å¯ç”¨æ¨¡å‹ç‰ˆæœ¬ï¼š{sorted(self.available_versions)}")
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ç‰ˆæœ¬ä¿¡æ¯å¤±è´¥ï¼š{str(e)}ï¼Œå¯ç”¨ç‰ˆæœ¬å°†åŠ¨æ€æ¢æµ‹")
            self.available_versions = set()

    def add_chunks_with_version(self, chunks: List[Dict[str, Any]], embeddings: List[List[float]],
                                model_version: str) -> None:
        if not model_version:
            raise ValueError("æ¨¡å‹ç‰ˆæœ¬ä¸èƒ½ä¸ºç©ºï¼Œè¯·æŒ‡å®šæœ‰æ•ˆçš„ç‰ˆæœ¬æ ‡è¯†")

        if not MILVUS_AVAILABLE or not self.collection:
            self.in_memory_chunks.extend(chunks)
            self.in_memory_embeddings.extend(embeddings)
            self.in_memory_versions.extend([model_version] * len(chunks))
            self.available_versions.add(model_version)
            print(f"âš ï¸ å†…å­˜æ¨¡å¼ï¼šæ·»åŠ  {len(chunks)} ä¸ªChunkï¼ˆç‰ˆæœ¬ï¼š{model_version}ï¼‰")
            return

        data = []
        for idx, (chunk, emb) in enumerate(zip(chunks, embeddings)):
            chunk_id = f"chunk_{uuid.uuid4().hex[:16]}"
            data.append([
                chunk_id,
                emb,
                chunk["content"][:4095],
                chunk["metadata"],
                model_version
            ])

        insert_result = self.collection.insert(data=data, using="default")
        self.collection.flush()
        self.available_versions.add(model_version)
        print(
            f"âœ… å‘Milvusæ’å…¥ {len(insert_result.primary_keys)} ä¸ªChunkï¼ˆç‰ˆæœ¬ï¼š{model_version}ï¼Œæ€»æ•°é‡ï¼š{self.collection.num_entities}ï¼‰")

    def search_with_version(self, query_embedding: List[float], model_version: str,
                            top_k: int = 3, nprobe: int = 32) -> Tuple[List[Dict[str, Any]], bool]:
        start_time = time.time()
        version_matched = True

        if model_version not in self.available_versions:
            if MILVUS_AVAILABLE and self.collection:
                try:
                    res = self.collection.query(
                        expr=f'embedding_model_version == "{model_version}"',
                        limit=1
                    )
                    if res:
                        self.available_versions.add(model_version)
                        version_matched = True
                    else:
                        version_matched = False
                except Exception as e:
                    print(f"âš ï¸ ç‰ˆæœ¬æ¢æµ‹å¤±è´¥ï¼š{str(e)}ï¼Œå°†è¿”å›å…¨ç‰ˆæœ¬ç»“æœ")
                    version_matched = False
            else:
                version_matched = model_version in self.in_memory_versions

        if not version_matched:
            print(f"âš ï¸ æ¨¡å‹ç‰ˆæœ¬[{model_version}]åœ¨Milvusä¸­ä¸å­˜åœ¨ï¼Œå°†è¿”å›å…¨ç‰ˆæœ¬ç»“æœï¼ˆå¯èƒ½ä¸åŒ¹é…ï¼‰")
            expr = "1==1"
        else:
            expr = f'embedding_model_version == "{model_version}"'

        if not MILVUS_AVAILABLE or not self.collection:
            if version_matched:
                version_indices = [i for i, v in enumerate(self.in_memory_versions) if v == model_version]
                filtered_emb = [self.in_memory_embeddings[i] for i in version_indices]
                filtered_chunks = [self.in_memory_chunks[i] for i in version_indices]
            else:
                filtered_emb = self.in_memory_embeddings
                filtered_chunks = self.in_memory_chunks

            from numpy.linalg import norm
            import numpy as np
            if not filtered_emb:
                return [], version_matched
            emb_matrix = np.array(filtered_emb)
            query_emb = np.array(query_embedding)
            sims = emb_matrix @ query_emb / (norm(emb_matrix, axis=1) * norm(query_emb) + 1e-8)
            indices = sims.argsort()[::-1][:top_k]
            results = [
                {
                    "id": f"memory_chunk_{idx}",
                    "content": filtered_chunks[i]["content"],
                    "metadata": filtered_chunks[i]["metadata"],
                    "similarity": round(sims[i], 4),
                    "model_version": self.in_memory_versions[version_indices[i]] if version_matched else
                    self.in_memory_versions[i]
                }
                for i in indices
            ]
            print(
                f"âš ï¸ å†…å­˜æ¨¡å¼æ£€ç´¢å®Œæˆï¼ˆç‰ˆæœ¬åŒ¹é…ï¼š{version_matched}ï¼‰ï¼Œè€—æ—¶ï¼š{round((time.time() - start_time) * 1000, 2)}ms")
            return results, version_matched

        search_params = {"metric_type": "IP", "params": {"nprobe": nprobe}}
        search_result = self.collection.search(
            data=[query_embedding],
            anns_field="embedding",
            param=search_params,
            limit=top_k,
            expr=expr,
            output_fields=["content", "metadata", "embedding_model_version"],
            using="default"
        )

        chunks = []
        for hits in search_result:
            for hit in hits:
                chunks.append({
                    "id": hit.id,
                    "content": hit.entity.get("content"),
                    "metadata": hit.entity.get("metadata"),
                    "similarity": round(hit.score, 4),
                    "model_version": hit.entity.get("embedding_model_version", "unknown")
                })

        search_duration = round((time.time() - start_time) * 1000, 2)
        print(f"âœ… Milvusæ£€ç´¢å®Œæˆï¼ˆç‰ˆæœ¬ï¼š{model_version}ï¼ŒåŒ¹é…ï¼š{version_matched}ï¼Œtop-{top_k}ï¼‰ï¼Œè€—æ—¶ï¼š{search_duration}ms")
        return chunks, version_matched

    def get_compatible_versions(self) -> List[str]:
        return sorted(self.available_versions)


# ===================== 6. å…œåº•æ£€ç´¢å·¥å…· =====================
class KeywordFallbackRetriever:
    def __init__(self, all_chunks: List[Dict[str, Any]] = None):
        self.all_chunks = all_chunks or []
        self.tfidf_vectorizer = None
        self.chunk_vectors = None
        self._build_tfidf_index()
        self._build_rule_map()

    def _build_tfidf_index(self) -> None:
        if not TFIDF_AVAILABLE or not self.all_chunks:
            print("âš ï¸ TF-IDFç´¢å¼•æ„å»ºæ¡ä»¶ä¸è¶³ï¼ˆç¼ºå°‘ä¾èµ–æˆ–æ•°æ®ï¼‰")
            return

        try:
            chunk_texts = [chunk["content"] for chunk in self.all_chunks]
            self.tfidf_vectorizer = TfidfVectorizer(
                stop_words="english",
                ngram_range=(1, 3),
                max_features=10000
            )
            self.chunk_vectors = self.tfidf_vectorizer.fit_transform(chunk_texts)
            print(f"âœ… TF-IDFç´¢å¼•æ„å»ºå®Œæˆï¼ˆ{len(self.all_chunks)}ä¸ªChunkï¼Œ{len(self.tfidf_vectorizer.vocabulary_)}ä¸ªç‰¹å¾ï¼‰")
        except Exception as e:
            print(f"âš ï¸ TF-IDFç´¢å¼•æ„å»ºå¤±è´¥ï¼š{str(e)}")
            self.tfidf_vectorizer = None
            self.chunk_vectors = None

    def _build_rule_map(self) -> None:
        self.rule_map = {
            "å‡€åˆ©æ¶¦": lambda c: "å‡€åˆ©æ¶¦" in c["content"] or "net profit" in c["content"].lower(),
            "è¥ä¸šæ”¶å…¥": lambda c: "è¥ä¸šæ”¶å…¥" in c["content"] or "revenue" in c["content"].lower(),
            "åŒæ¯”å¢é•¿ç‡": lambda c: "åŒæ¯”å¢é•¿" in c["content"] or "year-on-year" in c["content"].lower(),
            "èµ„äº§å‡å€¼æŸå¤±": lambda c: "èµ„äº§å‡å€¼" in c["content"] or "impairment" in c["content"].lower(),
            "æ¯›åˆ©ç‡": lambda c: "æ¯›åˆ©ç‡" in c["content"] or "gross margin" in c["content"].lower()
        }
        self.number_pattern = re.compile(r"\d+(\.\d+)?(%|å…ƒ|ä¸‡å…ƒ|äº¿å…ƒ|å¹´|æœˆ|æ—¥)")
        print("âœ… è§„åˆ™åŒ¹é…æ˜ å°„æ„å»ºå®Œæˆ")

    def update_chunks(self, chunks: List[Dict[str, Any]]) -> None:
        self.all_chunks = chunks
        self._build_tfidf_index()
        self._build_rule_map()

    def retrieve(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        start_time = time.time()
        if not self.all_chunks:
            print("âŒ å…œåº•æ£€ç´¢æ— å¯ç”¨Chunkæ•°æ®")
            return []

        rule_matched = []
        for chunk in self.all_chunks:
            match_score = 0
            for keyword, rule_func in self.rule_map.items():
                if keyword in query and rule_func(chunk):
                    match_score += 2
            query_numbers = set(self.number_pattern.findall(query))
            chunk_numbers = set(self.number_pattern.findall(chunk["content"]))
            if query_numbers & chunk_numbers:
                match_score += 1.5

            if match_score > 0:
                rule_matched.append((chunk, match_score))

        tfidf_matched = []
        if TFIDF_AVAILABLE and self.tfidf_vectorizer and self.chunk_vectors is not None:
            try:
                query_vector = self.tfidf_vectorizer.transform([query])
                similarities = cosine_similarity(query_vector, self.chunk_vectors)[0]
                tfidf_candidates = [
                    (self.all_chunks[i], float(similarities[i]))
                    for i in similarities.argsort()[::-1][:10]
                    if self.all_chunks[i] not in [c for c, _ in rule_matched]
                ]
                tfidf_matched = [(c, s * 1.0) for c, s in tfidf_candidates]
            except Exception as e:
                print(f"âš ï¸ TF-IDFåŒ¹é…å¤±è´¥ï¼š{str(e)}")

        all_candidates = rule_matched + tfidf_matched
        if not all_candidates:
            all_candidates = [(chunk, 0.1) for chunk in self.all_chunks[:top_k]]

        all_candidates.sort(key=lambda x: (-x[1], len(x[0]["content"])))
        final_results = [
            {
                **c,
                "similarity": round(score, 4),
                "retrieval_type": "keyword_fallback",
                "model_version": "fallback"
            }
            for c, score in all_candidates[:top_k]
        ]

        print(f"âœ… å…œåº•å…³é”®è¯æ£€ç´¢å®Œæˆï¼ˆ{len(final_results)}ä¸ªç»“æœï¼‰ï¼Œè€—æ—¶ï¼š{round((time.time() - start_time) * 1000, 2)}ms")
        return final_results


# ===================== 7. æµé‡åˆ†å±‚è·¯ç”±å·¥å…· =====================
class LLMClientRouter:
    def __init__(self):
        self.local_config = {
            "api_key": os.getenv("LOCAL_LLM_API_KEY", os.getenv("LOCAL_API_KEY")),
            "base_url": os.getenv("LOCAL_LLM_BASE_URL", os.getenv("LOCAL_BASE_URL")),
            "model": os.getenv("LOCAL_LLM_MODEL", os.getenv("LOCAL_TEXT_MODEL", "qwen2.5-7b-instruct")),
            "desc": "æœ¬åœ°A6000 GPU"
        }
        self.cloud_config = {
            "api_key": os.getenv("CLOUD_LLM_API_KEY"),
            "base_url": os.getenv("CLOUD_LLM_BASE_URL"),
            "model": os.getenv("CLOUD_LLM_MODEL", "qwen2.5-72b-instruct"),
            "desc": "äº‘ç«¯å¤§æ¨¡å‹æœåŠ¡"
        }

        self._validate_config()
        self.off_peak_hours = (22, 8)
        print(f"âœ… LLMè·¯ç”±åˆå§‹åŒ–å®Œæˆï¼š")
        print(
            f"   - éå³°å€¼æ—¶æ®µï¼ˆ{self.off_peak_hours[0]}:00-{self.off_peak_hours[1]}:00ï¼‰ä½¿ç”¨ï¼š{self.local_config['desc']}")
        print(f"   - å³°å€¼æ—¶æ®µï¼ˆ{self.off_peak_hours[1]}:00-{self.off_peak_hours[0]}:00ï¼‰ä½¿ç”¨ï¼š{self.cloud_config['desc']}")

    def _validate_config(self) -> None:
        if not (self.local_config["api_key"] and self.local_config["base_url"]):
            raise ValueError("æœ¬åœ°LLMé…ç½®ä¸å®Œæ•´ï¼Œè¯·åœ¨.envä¸­è®¾ç½®LOCAL_LLM_API_KEYå’ŒLOCAL_LLM_BASE_URL")
        if not (self.cloud_config["api_key"] and self.cloud_config["base_url"]):
            raise ValueError("äº‘ç«¯LLMé…ç½®ä¸å®Œæ•´ï¼Œè¯·åœ¨.envä¸­è®¾ç½®CLOUD_LLM_API_KEYå’ŒCLOUD_LLM_BASE_URL")

    def _is_off_peak(self) -> bool:
        current_hour = time.localtime().tm_hour
        start_off_peak, end_off_peak = self.off_peak_hours
        return current_hour >= start_off_peak or current_hour < end_off_peak

    def get_client(self) -> Tuple[OpenAI, Dict[str, str]]:
        if self._is_off_peak():
            client = OpenAI(
                api_key=self.local_config["api_key"],
                base_url=self.local_config["base_url"]
            )
            meta = {
                "client_type": "local",
                "desc": self.local_config["desc"],
                "model": self.local_config["model"],
                "hour": time.localtime().tm_hour,
                "is_off_peak": True
            }
        else:
            client = OpenAI(
                api_key=self.cloud_config["api_key"],
                base_url=self.cloud_config["base_url"]
            )
            meta = {
                "client_type": "cloud",
                "desc": self.cloud_config["desc"],
                "model": self.cloud_config["model"],
                "hour": time.localtime().tm_hour,
                "is_off_peak": False
            }

        print(f"ğŸ”€ LLMè·¯ç”±ï¼šå½“å‰{meta['hour']}ç‚¹ï¼Œä½¿ç”¨{meta['client_type']}å®¢æˆ·ç«¯ï¼ˆ{meta['desc']}ï¼‰")
        return client, meta

    def get_model(self) -> str:
        return self.local_config["model"] if self._is_off_peak() else self.cloud_config["model"]


# ===================== 8. åµŒå…¥æ¨¡å‹ï¼ˆç‰ˆæœ¬ç®¡ç†ï¼‰ =====================
class VersionedEmbeddingModel:
    def __init__(self, batch_size: int = 64):
        self.current_version = os.getenv("EMBEDDING_MODEL_VERSION", "bge-m3-v202405")
        self.model_configs = self._load_version_configs()
        self.batch_size = batch_size

        if self.current_version not in self.model_configs:
            print(f"âš ï¸ å½“å‰é…ç½®çš„æ¨¡å‹ç‰ˆæœ¬[{self.current_version}]ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤ç‰ˆæœ¬")
            self.current_version = next(iter(self.model_configs.keys())) if self.model_configs else "unknown"

        self._load_model(self.current_version)
        print(f"âœ… åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼ˆç‰ˆæœ¬ï¼š{self.current_version}ï¼Œæ¨¡å‹ï¼š{self.embedding_model}ï¼‰")

    def _load_version_configs(self) -> Dict[str, Dict[str, str]]:
        version_configs = {}
        base_config = {
            "api_key": os.getenv("EMBEDDING_API_KEY", os.getenv("LOCAL_API_KEY")),
            "base_url": os.getenv("EMBEDDING_BASE_URL", os.getenv("LOCAL_BASE_URL")),
            "model_name": os.getenv("EMBEDDING_MODEL", "BAAI/bge-m3")
        }
        version_configs["bge-m3-v202405"] = base_config

        extra_versions = os.getenv("EXTRA_EMBEDDING_VERSIONS", "").split(",") if os.getenv(
            "EXTRA_EMBEDDING_VERSIONS") else []
        for ver in extra_versions:
            ver = ver.strip()
            if not ver:
                continue
            version_configs[ver] = {
                "api_key": os.getenv(f"EMBEDDING_API_KEY_{ver.upper()}", base_config["api_key"]),
                "base_url": os.getenv(f"EMBEDDING_BASE_URL_{ver.upper()}", base_config["base_url"]),
                "model_name": os.getenv(f"EMBEDDING_MODEL_{ver.upper()}", base_config["model_name"])
            }

        print(f"âœ… åŠ è½½åµŒå…¥æ¨¡å‹ç‰ˆæœ¬é…ç½®ï¼š{sorted(version_configs.keys())}")
        return version_configs

    def _load_model(self, version: str) -> None:
        if version not in self.model_configs:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç‰ˆæœ¬ï¼š{version}ï¼Œå¯ç”¨ç‰ˆæœ¬ï¼š{sorted(self.model_configs.keys())}")

        config = self.model_configs[version]
        self.api_key = config["api_key"]
        self.base_url = config["base_url"]
        self.embedding_model = config["model_name"]

        if not self.api_key or not self.base_url:
            raise ValueError(f"æ¨¡å‹ç‰ˆæœ¬[{version}]é…ç½®ä¸å®Œæ•´ï¼Œç¼ºå°‘api_keyæˆ–base_url")

    def switch_version(self, target_version: str) -> bool:
        if target_version == self.current_version:
            print(f"â„¹ï¸ å½“å‰å·²ä½¿ç”¨æ¨¡å‹ç‰ˆæœ¬[{target_version}]ï¼Œæ— éœ€åˆ‡æ¢")
            return True
        if target_version not in self.model_configs:
            print(f"âŒ æ— æ³•åˆ‡æ¢åˆ°ç‰ˆæœ¬[{target_version}]ï¼Œå¯ç”¨ç‰ˆæœ¬ï¼š{sorted(self.model_configs.keys())}")
            return False

        try:
            self._load_model(target_version)
            self.current_version = target_version
            print(f"âœ… æˆåŠŸåˆ‡æ¢åµŒå…¥æ¨¡å‹ç‰ˆæœ¬ï¼š{target_version}ï¼ˆæ¨¡å‹ï¼š{self.embedding_model}ï¼‰")
            return True
        except Exception as e:
            print(f"âŒ åˆ‡æ¢ç‰ˆæœ¬[{target_version}]å¤±è´¥ï¼š{str(e)}ï¼Œä¿æŒå½“å‰ç‰ˆæœ¬[{self.current_version}]")
            return False

    def get_available_versions(self) -> List[str]:
        return sorted(self.model_configs.keys())

    def embed_texts(self, texts: List[str]) -> List[List[float]]:
        return get_text_embedding(
            texts,
            api_key=self.api_key,
            base_url=self.base_url,
            embedding_model=self.embedding_model,
            batch_size=self.batch_size
        )

    def embed_text(self, text: str) -> List[float]:
        return self.embed_texts([text])[0]


# ===================== 9. ä¼˜åŒ–çš„Promptæ„é€ å‡½æ•°ï¼ˆç”Ÿæˆå¯æ§æ€§æ ¸å¿ƒï¼‰ =====================
def build_prompt(query, context_chunks):
    context = "\n".join(
        [f"å†…å®¹ï¼š{c['content']}\næ¥æºï¼š{c['metadata']['file_name']} P{c['metadata']['page']}" for c in context_chunks])
    return f"""
    ä»»åŠ¡ï¼šåŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼Œä¸¥æ ¼éµå¾ªè§„åˆ™ï¼š
    1. ç­”æ¡ˆå¿…é¡»å®Œå…¨æ¥è‡ªä¸Šä¸‹æ–‡ï¼Œä¸å¾—ç¼–é€ ä¿¡æ¯ï¼›
    2. è‹¥ä¸Šä¸‹æ–‡ä¸è¶³ï¼Œç›´æ¥å›å¤"æ— æ³•å›ç­”"ï¼›
    3. å¿…é¡»ç”¨JSONæ ¼å¼è¾“å‡ºï¼ŒåŒ…å«"answer"å’Œ"source"å­—æ®µï¼ˆsourceä¸ºæ¥æºåˆ—è¡¨ï¼‰ã€‚

    ä¸Šä¸‹æ–‡ï¼š
    {context}

    é—®é¢˜ï¼š{query}

    è¾“å‡ºç¤ºä¾‹ï¼š
    {{
        "answer": "2023å¹´å‡€åˆ©æ¶¦ä¸º1000ä¸‡å…ƒ",
        "source": ["XXè´¢æŠ¥.pdf P15"]
    }}
    """


# ===================== 10. æ ¸å¿ƒRAGç±»ï¼ˆæ•´åˆä¼˜åŒ–ï¼‰ =====================
class SimpleRAG:
    def __init__(self, chunk_json_path: str, batch_size: int = 32,
                 max_chunk_tokens: int = 512, slide_step: int = 200,
                 milvus_dim: int = 1536):
        self.enable_finance_mode = os.getenv('ENABLE_FINANCE_MODE', 'false').lower() == 'true'
        self.enable_fp8 = os.getenv('ENABLE_FP8_INFERENCE', 'false').lower() == 'true'
        self.max_chunk_tokens = max_chunk_tokens
        self.slide_step = slide_step

        self.audit_logger = AuditLogManager(
            redis_host=os.getenv("REDIS_HOST", "localhost"),
            redis_port=int(os.getenv("REDIS_PORT", 6379)),
            redis_db=int(os.getenv("REDIS_DB", 0))
        )

        self.loader = PageChunkLoader(chunk_json_path)
        self.raw_chunks = self.loader.load_chunks()

        self.embedding_model = VersionedEmbeddingModel(batch_size=batch_size)
        self.current_embedding_version = self.embedding_model.current_version

        self.vector_store = MilvusVectorStore(
            collection_name=os.getenv("MILVUS_COLLECTION", "rag_finance_chunks"),
            dim=milvus_dim
        )

        self.reranker = FlagReranker(
            'BAAI/bge-reranker-large',
            use_fp16=True,
            devices=["cuda:0"] if torch.cuda.is_available() else ["cpu"]
        )
        print("âœ… BGEé‡æ’æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

        self.llm_router = LLMClientRouter()
        self.fallback_retriever = KeywordFallbackRetriever()
        self.bm25_retriever = None  # å»¶è¿Ÿåˆå§‹åŒ–ï¼Œç­‰å¾…processed_chunks

        if self.enable_finance_mode:
            try:
                from finance_rag_extension import FinanceVectorStore
                self.vector_store = FinanceVectorStore()
                print("âœ… å¯ç”¨é‡‘èç‰ˆå‘é‡å­˜å‚¨")
            except ImportError:
                print("âš ï¸ æœªæ‰¾åˆ°finance_rag_extensionï¼Œä½¿ç”¨é»˜è®¤Milvuså‘é‡å­˜å‚¨")

    def setup(self):
        print("=" * 50)
        print("å¼€å§‹RAGç³»ç»Ÿåˆå§‹åŒ–...")

        print("1. å¤„ç†è¶…é•¿Chunkï¼ˆæ»‘åŠ¨çª—å£+æ•°å­—ä¼˜å…ˆæˆªæ–­ï¼‰...")
        processed_chunks = []
        for raw_chunk in tqdm(self.raw_chunks, desc="   Chunké¢„å¤„ç†"):
            sub_chunks = process_long_chunk(
                chunk=raw_chunk,
                max_tokens=self.max_chunk_tokens,
                slide_step=self.slide_step
            )
            processed_chunks.extend(sub_chunks)
        print(f"   é¢„å¤„ç†åæ€»Chunkæ•°: {len(processed_chunks)}ï¼ˆå«å­Chunkï¼‰")

        # åˆå§‹åŒ–BM25æ£€ç´¢å™¨ï¼ˆæ ¸å¿ƒæ–°å¢ï¼‰
        self.bm25_retriever = BM25Retriever(processed_chunks)
        print(f"âœ… BM25æ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆï¼ˆ{len(processed_chunks)}ä¸ªChunkï¼‰")

        self.fallback_retriever.update_chunks(processed_chunks)

        print(f"2. ç”Ÿæˆæ–‡æœ¬åµŒå…¥ï¼ˆç‰ˆæœ¬ï¼š{self.current_embedding_version}ï¼‰...")
        start_embed = time.time()
        embeddings = self.embedding_model.embed_texts([c['content'] for c in processed_chunks])
        embed_duration = round((time.time() - start_embed) * 1000, 2)
        print(f"   åµŒå…¥ç”Ÿæˆå®Œæˆï¼ˆ{len(embeddings)}ä¸ªå‘é‡ï¼‰ï¼Œè€—æ—¶ï¼š{embed_duration}ms")

        print("3. å‘å‘é‡åº“å†™å…¥å¸¦ç‰ˆæœ¬çš„Chunk...")
        self.vector_store.add_chunks_with_version(
            chunks=processed_chunks,
            embeddings=embeddings,
            model_version=self.current_embedding_version
        )

        print("âœ… RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
        print(f"   - å‘é‡åº“ç±»å‹ï¼š{'Milvus' if MILVUS_AVAILABLE and self.vector_store.collection else 'å†…å­˜æ¨¡å¼'}")
        print(
            f"   - åµŒå…¥æ¨¡å‹ç‰ˆæœ¬ï¼š{self.current_embedding_version}ï¼ˆæ”¯æŒç‰ˆæœ¬ï¼š{self.embedding_model.get_available_versions()}ï¼‰")
        print(f"   - æ··åˆæ£€ç´¢ï¼šå‘é‡æ£€ç´¢ + BM25ï¼ˆRRFèåˆï¼‰")
        print(f"   - é‡æ’åºæ¨¡å‹ï¼šBAAI/bge-reranker-largeï¼ˆ{'GPU' if torch.cuda.is_available() else 'CPU'}æ¨¡å¼ï¼‰")
        print(f"   - å…œåº•æ£€ç´¢ï¼š{'å¯ç”¨' if TFIDF_AVAILABLE else 'ç¦ç”¨'}")
        print(f"   - é‡‘èæ¨¡å¼ï¼š{'å¯ç”¨' if self.enable_finance_mode else 'ç¦ç”¨'}")
        print("=" * 50)

    def switch_embedding_version(self, target_version: str) -> bool:
        embed_switch_ok = self.embedding_model.switch_version(target_version)
        if embed_switch_ok:
            self.current_embedding_version = target_version
            self.vector_store._load_all_versions()
            print(
                f"âœ… ç‰ˆæœ¬åˆ‡æ¢å®Œæˆï¼Œå½“å‰åµŒå…¥ç‰ˆæœ¬ï¼š{self.current_embedding_version}ï¼Œå‘é‡åº“å¯ç”¨ç‰ˆæœ¬ï¼š{sorted(self.vector_store.available_versions)}")
        return embed_switch_ok

    def retrieve_and_rerank(self, query: str, top_k: int = 3, candidate_top_k: int = 20) -> Tuple[
        List[Dict[str, Any]], str, bool]:
        start_total = time.time()
        print("\n" + "=" * 50)
        print(f"å¼€å§‹å¤„ç†æŸ¥è¯¢ï¼š{query[:50]}...")

        print(f"1. ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼ˆç‰ˆæœ¬ï¼š{self.current_embedding_version}ï¼‰...")
        start_embed = time.time()
        q_emb = self.embedding_model.embed_text(query)
        embed_duration = round((time.time() - start_embed) * 1000, 2)
        print(f"   æŸ¥è¯¢å‘é‡ç”Ÿæˆè€—æ—¶ï¼š{embed_duration}ms")

        # æ··åˆæ£€ç´¢ï¼ˆå‘é‡+BM25+RRFèåˆï¼‰
        print(f"2. æ‰§è¡Œæ··åˆæ£€ç´¢ï¼ˆå‘é‡+BM25ï¼ŒRRFèåˆï¼‰...")
        hybrid_results, version_matched = hybrid_retrieve(
            query=query,
            vector_retriever=q_emb,
            bm25_retriever=self.bm25_retriever,
            vector_store=self.vector_store,
            embedding_version=self.current_embedding_version,
            top_k=candidate_top_k,
            candidate_top_k=candidate_top_k
        )

        # æ£€æŸ¥æ˜¯å¦éœ€è¦å…œåº•
        retrieval_type = "hybrid"
        final_candidates = hybrid_results
        if len(hybrid_results) < candidate_top_k // 2:
            print(f"âš ï¸ æ··åˆæ£€ç´¢ç»“æœä¸è¶³ï¼ˆ{len(hybrid_results)}ä¸ªï¼‰ï¼Œè§¦å‘å…œåº•æ£€ç´¢")
            fallback_results = self.fallback_retriever.retrieve(query, top_k=candidate_top_k)
            hybrid_ids = {res["id"] for res in hybrid_results}
            combined_results = hybrid_results + [res for res in fallback_results if res["id"] not in hybrid_ids]
            final_candidates = combined_results[:candidate_top_k]
            retrieval_type = "hybrid+fallback"

        # BGEé‡æ’
        print(f"3. ä½¿ç”¨BGEé‡æ’æ¨¡å‹å¯¹{len(final_candidates)}ä¸ªå€™é€‰ç»“æœç²¾æ’åº...")
        start_rerank = time.time()
        pairs = [(query, chunk["content"]) for chunk in final_candidates]
        scores = self.reranker.compute_score(pairs)
        reranked_chunks = [
            chunk for _, chunk in sorted(zip(scores, final_candidates), key=lambda x: x[0], reverse=True)
        ][:top_k]
        rerank_duration = round((time.time() - start_rerank) * 1000, 2)
        print(f"   é‡æ’å®Œæˆï¼Œè€—æ—¶ï¼š{rerank_duration}ms")

        total_duration = round((time.time() - start_total) * 1000, 2)
        print(f"âœ… æŸ¥è¯¢å®Œæˆï¼ˆæ£€ç´¢ç±»å‹ï¼š{retrieval_type}ï¼Œç‰ˆæœ¬åŒ¹é…ï¼š{version_matched}ï¼‰ï¼Œæ€»è€—æ—¶ï¼š{total_duration}ms")
        print("=" * 50)

        return reranked_chunks, retrieval_type, version_matched

    def generate_answer(self, question: str, top_k: int = 3, user_id: Optional[str] = None) -> Dict[str, Any]:
        current_client_meta = self.llm_router.get_client()[1]
        log_metadata = {
            "user_id": user_id,
            "top_k": top_k,
            "llm_client_type": current_client_meta["client_type"],
            "llm_model": current_client_meta["model"],
            "embedding_model_version": self.current_embedding_version,
            "vector_store_type": "Milvus" if MILVUS_AVAILABLE and self.vector_store.collection else "Memory"
        }
        log_id = self.audit_logger.create_pre_log(question, log_metadata)
        print(f"ğŸ“Œ ç”Ÿæˆå®¡è®¡æ—¥å¿—ID: {log_id}")

        try:
            retrieved_chunks, retrieval_type, version_matched = self.retrieve_and_rerank(
                question, top_k=top_k, candidate_top_k=20
            )

            # æ„å»ºä¸Šä¸‹æ–‡
            context_chunks = []
            for chunk in retrieved_chunks:
                context_chunks.append({
                    "content": chunk["content"],
                    "metadata": chunk["metadata"]
                })

            # ä½¿ç”¨ä¼˜åŒ–åçš„Promptï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼‰
            prompt = build_prompt(question, context_chunks)

            # è°ƒç”¨LLM
            print("5. è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”...")
            start_llm = time.time()
            llm_client, llm_meta = self.llm_router.get_client()
            request_kwargs = {
                "model": llm_meta["model"],
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ï¼Œä¸¥æ ¼æŒ‰ç…§æŒ‡å®šæ ¼å¼è¾“å‡ºç»“æœ"},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.2,
                "max_tokens": 1024,
                "stream": False
            }
            if self.enable_fp8 and llm_meta["client_type"] == "local":
                request_kwargs["extra_body"] = {"precision": "fp8"}
                print(f"   å¯ç”¨FP8ç²¾åº¦æ¨ç†ï¼ˆæœ¬åœ°GPUæ¨¡å¼ï¼‰")

            completion = llm_client.chat.completions.create(**request_kwargs)
            llm_duration = round((time.time() - start_llm) * 1000, 2)
            print(f"   å¤§æ¨¡å‹æ¨ç†è€—æ—¶ï¼š{llm_duration}msï¼ˆ{llm_meta['desc']}ï¼‰")

            # è§£æJSONç»“æœ
            raw = completion.choices[0].message.content.strip()
            try:
                answer_data = json.loads(raw)
                if not isinstance(answer_data, dict) or "answer" not in answer_data or "source" not in answer_data:
                    raise ValueError("è¾“å‡ºæ ¼å¼ç¼ºå°‘å¿…è¦å­—æ®µ")
                answer = answer_data["answer"]
                source = answer_data["source"]
                note = f"æ£€ç´¢ç±»å‹ï¼š{retrieval_type}ï¼Œæ¨¡å‹ç‰ˆæœ¬ï¼š{self.current_embedding_version}"
            except json.JSONDecodeError:
                answer = "æ— æ³•è§£ææ¨¡å‹è¾“å‡ºï¼ˆæ ¼å¼é”™è¯¯ï¼‰"
                source = []
                note = f"JSONè§£æå¤±è´¥ï¼ŒåŸå§‹è¾“å‡ºï¼š{raw[:100]}"
            except ValueError as e:
                answer = f"è¾“å‡ºæ ¼å¼é”™è¯¯ï¼š{str(e)}"
                source = []
                note = f"æ ¼å¼æ ¡éªŒå¤±è´¥ï¼ŒåŸå§‹è¾“å‡ºï¼š{raw[:100]}"

            # æ›´æ–°å®¡è®¡æ—¥å¿—
            self.audit_logger.update_log_with_answer(
                log_id=log_id,
                answer=json.dumps(answer_data, ensure_ascii=False) if 'answer_data' in locals() else answer,
                retrieval_chunks=retrieved_chunks,
                status="completed",
                model_version=self.current_embedding_version,
                retrieval_type=retrieval_type
            )
            asyncio.create_task(self.audit_logger.write_audit_log_to_db(log_id))

            total_duration = sum([
                chunk.get('retrieval_duration_ms', 0) for chunk in retrieved_chunks[:1]
            ]) + llm_duration
            return {
                "question": question,
                "answer": answer,
                "source": source,
                "note": note,
                "retrieval_chunks": retrieved_chunks,
                "audit_log_id": log_id,
                "llm_route_info": llm_meta,
                "version_info": {
                    "embedding_version": self.current_embedding_version,
                    "version_matched": version_matched,
                    "available_versions": sorted(self.vector_store.available_versions)
                },
                "performance_stats": {
                    "retrieval_duration_ms": round(total_duration - llm_duration, 2),
                    "llm_duration_ms": llm_duration,
                    "total_duration_ms": round(total_duration, 2)
                }
            }

        except Exception as e:
            error_msg = f"ç”Ÿæˆå›ç­”å¤±è´¥: {str(e)}"
            print(error_msg)
            self.audit_logger.update_log_with_answer(
                log_id=log_id,
                answer=error_msg,
                retrieval_chunks=[],
                status="failed",
                model_version=self.current_embedding_version,
                retrieval_type="error"
            )
            asyncio.create_task(self.audit_logger.write_audit_log_to_db(log_id))
            return {
                "question": question,
                "answer": error_msg,
                "source": [],
                "note": f"ç³»ç»Ÿé”™è¯¯ï¼Œæ¨¡å‹ç‰ˆæœ¬ï¼š{self.current_embedding_version}",
                "retrieval_chunks": [],
                "audit_log_id": log_id,
                "llm_route_info": current_client_meta,
                "version_info": {
                    "embedding_version": self.current_embedding_version,
                    "version_matched": False,
                    "available_versions": sorted(self.vector_store.available_versions)
                },
                "error": str(e)
            }


# ===================== 11. è¾…åŠ©ç±» =====================
class PageChunkLoader:
    def __init__(self, json_path: str):
        self.json_path = json_path

    def load_chunks(self) -> List[Dict[str, Any]]:
        with open(self.json_path, 'r', encoding='utf-8') as f:
            return json.load(f)


# ===================== 12. ä¸»å‡½æ•° =====================
if __name__ == '__main__':
    MAX_CHUNK_TOKENS = 512
    SLIDE_STEP = 200
    MILVUS_DIM = 1536
    TOP_K = 3
    CANDIDATE_TOP_K = 20

    chunk_json_path = os.path.join(os.path.dirname(__file__), 'all_pdf_page_chunks.json')
    rag = SimpleRAG(
        chunk_json_path=chunk_json_path,
        max_chunk_tokens=MAX_CHUNK_TOKENS,
        slide_step=SLIDE_STEP,
        milvus_dim=MILVUS_DIM
    )
    rag.setup()

    # æµ‹è¯•1ï¼šæ­£å¸¸æŸ¥è¯¢
    print("\n" + "=" * 60)
    print("ã€æµ‹è¯•1ï¼šæ­£å¸¸æŸ¥è¯¢ã€‘")
    test_question1 = "2023å¹´ç¬¬ä¸‰å­£åº¦çš„å‡€åˆ©æ¶¦æ˜¯å¤šå°‘ï¼Ÿ"
    result1 = rag.generate_answer(test_question1, top_k=TOP_K, user_id="test_user_001")
    print(f"é—®é¢˜ï¼š{test_question1}")
    print(f"å›ç­”ï¼š{result1['answer']}")
    print(f"æ¥æºï¼š{result1['source']}")
    print(
        f"ç‰ˆæœ¬ä¿¡æ¯ï¼šå½“å‰{result1['version_info']['embedding_version']}ï¼ŒåŒ¹é…ï¼š{result1['version_info']['version_matched']}")

    # æµ‹è¯•2ï¼šç‰ˆæœ¬åˆ‡æ¢åœºæ™¯
    print("\n" + "=" * 60)
    print("ã€æµ‹è¯•2ï¼šç‰ˆæœ¬åˆ‡æ¢ã€‘")
    target_version = "bge-m3-v202404" if "bge-m3-v202404" in rag.embedding_model.get_available_versions() else \
    rag.embedding_model.get_available_versions()[0]
    rag.switch_embedding_version(target_version)
    test_question2 = "è”é‚¦åˆ¶è¯2024å¹´è¥ä¸šæ”¶å…¥åŒæ¯”å¢é•¿ç‡ä¸ºå¤šå°‘ï¼Ÿ"
    result2 = rag.generate_answer(test_question2, top_k=TOP_K, user_id="test_user_002")
    print(f"é—®é¢˜ï¼š{test_question2}")
    print(f"å›ç­”ï¼š{result2['answer']}")
    print(f"æ¥æºï¼š{result2['source']}")

    # æ‰¹é‡æµ‹è¯•
    TEST_SAMPLE_NUM = 5
    test_path = os.path.join(os.path.dirname(__file__), 'datas/å¤šæ¨¡æ€RAGå›¾æ–‡é—®ç­”æŒ‘æˆ˜èµ›æµ‹è¯•é›†.json')
    if os.path.exists(test_path):
        print("\n" + "=" * 60)
        print(f"å¼€å§‹æ‰¹é‡æµ‹è¯•ï¼ˆ{TEST_SAMPLE_NUM}æ¡æ ·æœ¬ï¼‰...")
        with open(test_path, 'r', encoding='utf-8') as f:
            test_data = json.load(f)

        all_indices = list(range(len(test_data)))
        selected_indices = sorted(random.sample(all_indices, TEST_SAMPLE_NUM)) if len(
            test_data) > TEST_SAMPLE_NUM else all_indices

        for idx in selected_indices:
            data_item = test_data[idx]
            question = data_item['question']
            print(f"\n[{selected_indices.index(idx) + 1}/{len(selected_indices)}] å¤„ç†: {question[:30]}...")
            result = rag.generate_answer(question, top_k=TOP_K, user_id=f"batch_user_{idx}")
            print(f"å›ç­”ï¼š{result['answer'][:100]}...")
            print(f"æ¥æºï¼š{result['source']}")